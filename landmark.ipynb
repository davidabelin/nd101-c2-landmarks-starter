{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "landmark.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "f94fVpNB6p6A",
        "fYUnvhX9ZnzX",
        "eNv-TSitjifK",
        "rOSLoA5WjifL",
        "Wy-9HzYljifM",
        "xo5a5extjifN",
        "5SdFMX81jifN",
        "8Y_1Hnaub1ht"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT6Wia07jie5"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "## Project: Write an Algorithm for Landmark Classification\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
        "\n",
        "> **Note**: Once you have completed all the code implementations, you need to finalize your work by exporting the Jupyter Notebook as an HTML document. Before exporting the notebook to HTML, all the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to **File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
        "\n",
        "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
        "\n",
        ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
        "\n",
        "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this Jupyter notebook.\n",
        "\n",
        "---\n",
        "### Why We're Here\n",
        "\n",
        "Photo sharing and photo storage services like to have location data for each photo that is uploaded. With the location data, these services can build advanced features, such as automatic suggestion of relevant tags or automatic photo organization, which help provide a compelling user experience. Although a photo's location can often be obtained by looking at the photo's metadata, many photos uploaded to these services will not have location metadata available. This can happen when, for example, the camera capturing the picture does not have GPS or if a photo's metadata is scrubbed due to privacy concerns.\n",
        "\n",
        "If no location metadata for an image is available, one way to infer the location is to detect and classify a discernible landmark in the image. Given the large number of landmarks across the world and the immense volume of images that are uploaded to photo sharing services, using human judgement to classify these landmarks would not be feasible.\n",
        "\n",
        "In this notebook, you will take the first steps towards addressing this problem by building models to automatically predict the location of the image based on any landmarks depicted in the image. At the end of this project, your code will accept any user-supplied image as input and suggest the top k most relevant landmarks from 50 possible landmarks from across the world. The image below displays a potential sample output of your finished project.\n",
        "\n",
        "![Sample landmark classification output](images/sample_landmark_output.png)\n",
        "\n",
        "\n",
        "# The Road Ahead\n",
        "\n",
        "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
        "\n",
        "* [Step 0](#step0): Download Datasets and Install Python Modules\n",
        "* [Step 1](#step1): Create a CNN to Classify Landmarks (from Scratch)\n",
        "* [Step 2](#step2): Create a CNN to Classify Landmarks (using Transfer Learning)\n",
        "* [Step 3](#step3): Write Your Landmark Prediction Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWwhrq3ue6Dy"
      },
      "source": [
        "<a id='step0'></a>\n",
        "## Step 0: Download Datasets and Install Python Modules\n",
        "\n",
        "**Note: if you are using the Udacity workspace, *YOU CAN SKIP THIS STEP*. The dataset can be found in the `/data` folder and all required Python modules have been installed in the workspace.**\n",
        "\n",
        "Download the [landmark dataset](https://udacity-dlnfd.s3-us-west-1.amazonaws.com/datasets/landmark_images.zip).\n",
        "Unzip the folder and place it in this project's home directory, at the location `/landmark_images`. The landmark images are a subset of the **Google Landmarks Dataset v2**.\n",
        "\n",
        "Install the following Python modules:\n",
        "* cv2\n",
        "* matplotlib\n",
        "* numpy\n",
        "* PIL\n",
        "* torch\n",
        "* torchvision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emIdRzaxlEB7"
      },
      "source": [
        "import cv2\n",
        "import PIL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler\n",
        "\n",
        "import os\n",
        "import random as rd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4fdu02Y9sKv"
      },
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/Udacity/deep_learning/landmark_images.zip').extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDHJO8QSfhMH"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "#torch.cuda.empty_cache()\n",
        "#torch.cuda.list_gpu_processes()\n",
        "#torch.cuda.memory_snapshot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs7CallsuYuM"
      },
      "source": [
        "upload = True\n",
        "\n",
        "if upload:\n",
        "    try:\n",
        "        train_shapes = pd.read_csv('train_shapes.csv',header=0, index_col=0)\n",
        "        test_shapes = pd.read_csv('test_shapes.csv',header=0, index_col=0)\n",
        "    except FileNotFoundError:\n",
        "        print (\"Must upload files first.\")\n",
        "\n",
        "else:\n",
        "    #read train image file data:\n",
        "    train_path = './landmark_images/train/'\n",
        "    cat_path = [train_path + cat + '/' for cat in os.listdir(train_path)]\n",
        "    jpg_list = [[cp + jpg for jpg in os.listdir(cp)] for cp in cat_path]\n",
        "\n",
        "    train_images = []\n",
        "    for jl in jpg_list:\n",
        "        for jpg in jl:\n",
        "            train_images.append(jpg)\n",
        "\n",
        "    train_image_shapes = []\n",
        "    for imfile in train_images:\n",
        "        train_image_shapes.append(cv2.imread(imfile).shape)\n",
        "    heights = [shp[0] for shp in train_image_shapes]    \n",
        "    widths = [shp[1] for shp in train_image_shapes]\n",
        "    \n",
        "    train_images_df = pd.DataFrame(columns=['filepath','height','width'])\n",
        "    train_images_df['filepath'] = train_images\n",
        "    train_images_df['height'] = heights\n",
        "    train_images_df['width'] = widths\n",
        "    train_images_df.to_csv('train_shapes.csv')\n",
        "    \n",
        "    #read test image file data:\n",
        "    test_path = './landmark_images/test/'\n",
        "    cat_path = [test_path + cat + '/' for cat in os.listdir(test_path)]\n",
        "    jpg_list = [[cp + jpg for jpg in os.listdir(cp)] for cp in cat_path]\n",
        "\n",
        "    test_images = []\n",
        "    for jl in jpg_list:\n",
        "        for jpg in jl:\n",
        "            test_images.append(jpg)\n",
        "\n",
        "    test_image_shapes = []\n",
        "    for imfile in test_images:\n",
        "        test_image_shapes.append(cv2.imread(imfile).shape)\n",
        "    heights = [shp[0] for shp in test_image_shapes]    \n",
        "    widths = [shp[1] for shp in test_image_shapes] \n",
        "\n",
        "    test_images_df = pd.DataFrame(columns=['filepath','height','width'])\n",
        "    test_images_df['filepath'] = test_images\n",
        "    test_images_df['height'] = heights\n",
        "    test_images_df['width'] = widths\n",
        "    test_images_df.to_csv('test_shapes.csv')\n",
        "\n",
        "    #cleanup\n",
        "    del train_images, train_image_shapes, train_path\n",
        "    del cat_path, jpg_list, widths, heights\n",
        "    del test_images, test_image_shapes, test_path\n",
        "    #del train_images_df, test_images_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f94fVpNB6p6A"
      },
      "source": [
        "#### Data distractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ljRgXEmm1Ov"
      },
      "source": [
        "plt.scatter(train_shapes[\"height\"], train_shapes[\"width\"])\n",
        "plt.scatter(test_shapes[\"height\"], test_shapes[\"width\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySc7DeGLqCTY"
      },
      "source": [
        "critA = train_shapes[\"width\"] >= 700\n",
        "critB = train_shapes[\"height\"] >= 444\n",
        "#critC = train_shapes[\"height\"] <= 700.loc[critC]\n",
        "train_shapes[\"height\"].hist()\n",
        "train_shapes.loc[critA].loc[critB][\"height\"].hist()\n",
        "\n",
        "critD = test_shapes[\"width\"] >= 700\n",
        "critE = test_shapes[\"height\"] >= 444\n",
        "#critF = test_shapes[\"height\"] <= 700.loc[critF]\n",
        "test_shapes[\"height\"].hist()\n",
        "test_shapes.loc[critD].loc[critE][\"height\"].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I3p_hNnWVKh"
      },
      "source": [
        "train_shapes.shape,\\\n",
        "train_shapes.loc[critA].loc[critB].loc[critC].shape,\\\n",
        "test_shapes.shape,\\\n",
        "test_shapes.loc[critD].loc[critE].loc[critF].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54JK90x6Y3UT"
      },
      "source": [
        "train_shapes[\"height\"].min(), test_shapes[\"height\"].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuloVeLD6fvL"
      },
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC8GmaEYYW7l"
      },
      "source": [
        "#@ title Image transpose\n",
        "def bgr2rgb(im_bgr):\n",
        "    # because cv2.imread() returns BGR image format\n",
        "    im_rgb = im_bgr.copy()\n",
        "    im_rgb[:,:,0] = im_bgr[:,:,2]\n",
        "    im_rgb[:,:,1] = im_bgr[:,:,1]\n",
        "    im_rgb[:,:,2] = im_bgr[:,:,0]\n",
        "    return im_rgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8Fw9et59XIt"
      },
      "source": [
        "def quick_label(row):\n",
        "    label = row['filepath'][-40:-20] + \"\\nWidth: \" + str(row['width']) + \"  Height: \" + str(row['height'])\n",
        "    return label\n",
        "\n",
        "choices = rd.sample(list(train_shapes.index), 4)\n",
        "paths = [(train_shapes.iloc[s])['filepath'] for s in choices]\n",
        "labels = [quick_label(train_shapes.iloc[s]) for s in choices]\n",
        "imgs_bgr = [cv2.imread(p) for p in paths]\n",
        "imgs_rgb = [bgr2rgb(im) for im in imgs_bgr] \n",
        "\n",
        "fig, axes = plt.subplots(figsize=(18,16), nrows=2, ncols=2)\n",
        "axes = axes.reshape(4)\n",
        "for ii, im in enumerate(imgs_rgb):\n",
        "    ax0 = axes[ii] \n",
        "    ax0.set_title(labels[ii])\n",
        "    ax0.imshow(im)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOxSmkXzRumC"
      },
      "source": [
        "irgb = imgs_rgb[0]\n",
        "irgb.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HGgiGGqjie_"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Data Loaders for the Landmark Dataset\n",
        "\n",
        "Use the code cell below to create three separate [data loaders](http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader): one for training data, one for validation data, and one for test data. Randomly split the images located at `landmark_images/train` to create the train and validation data loaders, and use the images located at `landmark_images/test` to create the test data loader.\n",
        "\n",
        "All three of your data loaders should be accessible via a dictionary named `loaders_scratch`. Your train data loader should be at `loaders_scratch['train']`, your validation data loader should be at `loaders_scratch['valid']`, and your test data loader should be at `loaders_scratch['test']`.\n",
        "\n",
        "You may find [this documentation on custom datasets](https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder) to be a useful resource.  If you are interested in augmenting your training and/or validation data, check out the wide variety of [transforms](http://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transform)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mgOhgXEB2S1"
      },
      "source": [
        "# image directory location\n",
        "data_dir = '/content/landmark_images'\n",
        "\n",
        "# set target height and width for all images\n",
        "H = int(120)\n",
        "W = int(180)\n",
        "h0 = 210\n",
        "w0 = 280\n",
        "\n",
        "# Define transforms for the training data and testing data   \n",
        "# \n",
        "xform = False\n",
        "if xform:\n",
        "    train_transforms = transforms.Compose([ transforms.RandomHorizontalFlip(),\n",
        "                                            transforms.Resize((int(3.6*H),\n",
        "                                                                int(3.6*W))),\n",
        "                                            transforms.RandomCrop((int(2.1*H),\n",
        "                                                                int(2.1*W))),\n",
        "                                            transforms.RandomRotation((-10,10), \n",
        "                                                                    expand=True, \n",
        "                                                                    fill=128),\n",
        "                                            transforms.RandomCrop(int(1.2*H), \n",
        "                                                                  int(1.2*W)),\n",
        "                                            transforms.CenterCrop((H, W)),\n",
        "                                            transforms.ToTensor()]) \n",
        "else:\n",
        "    train_transforms = transforms.Compose([transforms.Resize((h0, w0)),\n",
        "                                           transforms.RandomCrop((H,W)),\n",
        "                                           transforms.ToTensor()])\n",
        "\n",
        "test_transforms = transforms.Compose([ transforms.Resize((H, W)),\n",
        "                                       transforms.ToTensor()])\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
        "\n",
        "######### from \"Multi-Layer Perceptron, MNIST\" notebook #######\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(0.2 * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "###############################################################\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, \n",
        "                                          batch_size=30,\n",
        "                                          sampler=train_sampler)\n",
        "\n",
        "validloader = torch.utils.data.DataLoader(train_data, \n",
        "                                          batch_size=30,\n",
        "                                          sampler=valid_sampler)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_data, \n",
        "                                         batch_size=30,\n",
        "                                         sampler=test_sampler)\n",
        "\n",
        "## Write data loaders for training, validation, and test sets\n",
        "## Specify appropriate transforms, and batch_sizes\n",
        "\n",
        "loaders_scratch = {'train': trainloader, 'valid': validloader, 'test': testloader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA6G3Qw9Behn"
      },
      "source": [
        "## if only want approx 600x800 images\n",
        "trainable_df = train_shapes.loc[critA].loc[critB]#.loc[critC]\n",
        "testable_df = test_shapes.loc[critD].loc[critE]#.loc[critF]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svbO5mb5-4oM"
      },
      "source": [
        "## Two class dicts\n",
        "\n",
        "classes = test_data.classes\n",
        "classes.sort()\n",
        "class2num = {c[3:]:int(c[:2]) for c in classes}\n",
        "\n",
        "classvals = list(set([(str(pair[0]),pair[1]) for pair in test_shapes[['labnum','label']].values]))\n",
        "num2class = {int(x):y for x,y in classvals}\n",
        "\n",
        "assert class2num[num2class[7]] == 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44DffOjKjifB"
      },
      "source": [
        "**Question 1:** Describe your chosen procedure for preprocessing the data. \n",
        "- How does your code resize the images (by cropping, stretching, etc)?  What size did you pick for the input tensor, and why?\n",
        "- Did you decide to augment the dataset?  If so, how (through translations, flips, rotations, etc)?  If not, why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wRtIsOHjifC"
      },
      "source": [
        "**Answer**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITQnn5K5jie-"
      },
      "source": [
        "<a id='step1'></a>\n",
        "## Step 1: Create a CNN to Classify Landmarks (from Scratch)\n",
        "\n",
        "In this step, you will create a CNN that classifies landmarks.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 20%.\n",
        "\n",
        "Although 20% may seem low at first glance, it seems more reasonable after realizing how difficult of a problem this is. Many times, an image that is taken at a landmark captures a fairly mundane image of an animal or plant, like in the following picture.\n",
        "\n",
        "<img src=\"images/train/00.Haleakala_National_Park/084c2aa50d0a9249.jpg\" alt=\"Bird in Haleakalā National Park\" style=\"width: 400px;\"/>\n",
        "\n",
        "Just by looking at that image alone, would you have been able to guess that it was taken at the Haleakalā National Park in Hawaii?\n",
        "\n",
        "An accuracy of 20% is significantly better than random guessing, which would provide an accuracy of just 2%. In Step 2 of this notebook, you will have the opportunity to greatly improve accuracy by using transfer learning to create a CNN.\n",
        "\n",
        "Remember that practice is far ahead of theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEcUnrYCjifC"
      },
      "source": [
        "### (IMPLEMENTATION) Visualize a Batch of Training Data\n",
        "\n",
        "Use the code cell below to retrieve a batch of images from your train data loader, display at least 5 images simultaneously, and label each displayed image with its class name (e.g., \"Golden Gate Bridge\").\n",
        "\n",
        "Visualizing the output of your data loader is a great way to ensure that your data loading and preprocessing are working as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-dUVJvn11Ni"
      },
      "source": [
        "# Custom channel transpose vs np.transpose(im, (1,2,0))\n",
        "def rechannel(img):\n",
        "    # img is channels first: shape (C, H, W)\n",
        "    # new_img is channels last: shape (H, W, C)\n",
        "    new_img = torch.zeros(img.shape[1], img.shape[2], 3)\n",
        "    for c in range(3):\n",
        "        new_img[:,:,c] = img[c, :, :]\n",
        "    return new_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWrT07-4jifD"
      },
      "source": [
        "\n",
        "## TODO: visualize a batch of the train data loader\n",
        "\n",
        "## the class names can be accessed at the `classes` attribute\n",
        "## of your dataset object (e.g., `train_dataset.classes`)\n",
        "\n",
        "try:\n",
        "    images, labels = train_data_iter.next()\n",
        "except NameError:\n",
        "    loader = loaders_scratch['train']\n",
        "    train_data_iter = iter(loader)\n",
        "    images, labels = train_data_iter.next()\n",
        "\n",
        "fig = plt.figure(figsize=(21,8),clear=True)\n",
        "axes = fig.subplots(nrows=2, ncols=4, sharex=True, sharey=True)\n",
        "axes = axes.flatten()\n",
        "for i, im in enumerate(images[0:8]):\n",
        "    ax = axes[i]\n",
        "    title = classes[labels[i].numpy()][3:]\n",
        "    ax.set_title(title)\n",
        "    axes[i].set_xticks([])\n",
        "    axes[i].set_yticks([])\n",
        "    ax.imshow(np.transpose(im, (1,2,0)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYBJPQbe4qr2"
      },
      "source": [
        "### View test images\n",
        "try:\n",
        "    images, labels = data_iter.next()\n",
        "except NameError:\n",
        "    data_iter = iter(testloader)\n",
        "    images, labels = data_iter.next()\n",
        "\n",
        "labels = labels.tolist()\n",
        "\n",
        "fig = plt.figure(figsize=(22,8.5))#, dpi=72)\n",
        "axes = fig.subplots(nrows=2, ncols=4)\n",
        "axes = axes.flatten()\n",
        "\n",
        "idx = rd.randrange(images.shape[0]-8)\n",
        "for i in range(idx,idx+8):\n",
        "    img = images[i]\n",
        "    img = np.transpose(img, (1,2,0))  #rechannel(img)\n",
        "    title = classes[labels[i]]\n",
        "    axes[i-idx].set_title(title[3:])\n",
        "    axes[i-idx].set_xticks([])\n",
        "    axes[i-idx].set_yticks([])\n",
        "    axes[i-idx].imshow(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miqR77sujifD"
      },
      "source": [
        "### Initialize use_cuda variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAfuRXGJjifE"
      },
      "source": [
        "# useful variable that tells us whether we should use the GPU\n",
        "if not use_cuda: use_cuda = torch.cuda.is_available()\n",
        "torch.cuda.list_gpu_processes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_94OXey5jifE"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
        "\n",
        "Use the next code cell to specify a [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/stable/optim.html).  Save the chosen loss function as `criterion_scratch`, and fill in the function `get_optimizer_scratch` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV9O3s9QjifF"
      },
      "source": [
        "## TODO: select loss function\n",
        "criterion_scratch = nn.CrossEntropyLoss()\n",
        "\n",
        "def get_optimizer_scratch(model):\n",
        "    ## TODO: select and return an optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3pBgngajifF"
      },
      "source": [
        "### (IMPLEMENTATION) Model Architecture\n",
        "\n",
        "Create a CNN to classify images of landmarks.  Use the template in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia2gNkXV10Rl"
      },
      "source": [
        "# Calculate a layer's output shape\n",
        "def output_volume(w, f, s, p):\n",
        "    #params:\n",
        "    #w = input volume (tuple)\n",
        "    #f = kernel size (tuple)\n",
        "    #s = stride (int)\n",
        "    #p = zero padding (int)\n",
        "    x = (w[0] - f[0] + 2*p)/s + 1\n",
        "    y = (w[1] - f[1] + 2*p)/s + 1\n",
        "    return (np.int(np.floor(x)), np.int(np.floor(y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAJ781t0jifF",
        "cellView": "form"
      },
      "source": [
        "#@title Original Net (no running)\n",
        "# define the CNN architecture\n",
        "if False:\n",
        "    class oNet(nn.Module):\n",
        "        ## TODO: choose an architecture, and complete the class\n",
        "        def __init__(self):\n",
        "            super(oNet, self).__init__()\n",
        "            ## Define layers of a CNN\n",
        "            # number of nodes in each hidden layer\n",
        "            nodes_1 = 2048\n",
        "            nodes_2 = 512\n",
        "            # input layer (H * W -> nodes_1)\n",
        "            self.fc1 = nn.Linear(H * W, nodes_1)\n",
        "            # hidden layer (nodes_1 -> nodes_2)\n",
        "            self.fc2 = nn.Linear(nodes_1, nodes_2)\n",
        "            # output layer (nodes_2 -> 10)\n",
        "            self.fc3 = nn.Linear(nodes_2, 10)\n",
        "            # dropout layers\n",
        "            self.dropout2 = nn.Dropout(0.2)\n",
        "            self.dropout3 = nn.Dropout(0.3)\n",
        "            self.dropout4 = nn.Dropout(0.4)\n",
        "            \n",
        "        def forward(self, x):\n",
        "            ## Define forward pass\n",
        "            # x is input image tensor\n",
        "            # flatten input\n",
        "            x = x.view(-1, H * W)\n",
        "            # first hidden layer\n",
        "            x = F.relu(self.fc1(x))\n",
        "            # first dropout layer\n",
        "            x = self.dropout3(x)\n",
        "            # second hidden layer\n",
        "            x = F.relu(self.fc2(x))\n",
        "            # second dropout layer\n",
        "            x = self.dropout2(x)\n",
        "            # output layer\n",
        "            x = self.fc3(x)\n",
        "            return x\n",
        "\n",
        "    #-#-# Do NOT modify the code below this line. #-#-#\n",
        "\n",
        "    # instantiate the CNN\n",
        "    model_scratch = Net()\n",
        "\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if use_cuda:\n",
        "        model_scratch.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_rpAuOk3t-x",
        "cellView": "form"
      },
      "source": [
        "print(\"input:\", W, H)\n",
        "\n",
        "(w,h) = output_volume((W,H),(50,50),5,25) #1 \n",
        "print(\"shrink1 out:\",w, h,\"\\tshrink2 in:\",w, h)\n",
        "\n",
        "(w,h) = output_volume((w,h),(30,30),1,3) #1 \n",
        "print(\"shrink2 out:\",w, h,\"\\t1rst in:\",w, h)\n",
        "\n",
        "(w,h) = output_volume((w,h),(11,11),1,1) #1 \n",
        "print(\"1rst out:\",w, h,\"\\t2nd in:\",w//2, h//2)\n",
        "\n",
        "(w,h) = output_volume((w//2,h//2),(7,7),1,1) #2\n",
        "print(\"2nd out:\",w, h,\"\\t3rd in:\",w//2, h//2)\n",
        "\n",
        "(w,h) = output_volume((w//2,h//2),(5,5),1,1) #3\n",
        "print(\"3rd out:\", w, h,\"\\t4th in:\",w//2, h//2)\n",
        "\n",
        "(w,h) = output_volume((w//2,h//2),(3,3),1,1) \n",
        "print(\"4th out:\", w, h,\"\\tfinal out:\",w, h)\n",
        "print(\"flattened:\", w*h)\n",
        "\n",
        "#@title original CNN architecture\n",
        "class cifar_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(cifar_Net, self).__init__()\n",
        "\n",
        "        # (160x120x3 image tensor) --> conv1 \n",
        "        # W,H --> 160,120 - 11 + 2 +1 = 152,112        \n",
        "        self.conv1 = nn.Conv2d(3, 20, 11, padding=1)\n",
        "        #  --> pool /2 --> (76x56x20) --> conv2\n",
        "        self.conv2 = nn.Conv2d(20, 30, 7, padding=1)\n",
        "        # 72,52 - 7 + 2 + 1 = 72,52\n",
        "        # --> pool /2 -->  (36x26x30) --> conv3\n",
        "        self.conv3 = nn.Conv2d(30, 40, 5, padding=1)\n",
        "        # 36,26 - 5 + 2 + 1 = 34,24\n",
        "        # --> pool /2 --> 17,12 (17x12x40) --> conv4\n",
        "        self.conv4 = nn.Conv2d(40, 50, 3, padding=1)\n",
        "        # 17,12 - 3 + 2 + 1 = 17,12\n",
        "        # (17x12x50)=10200 --> flatten\n",
        "\n",
        "        # max pooling --> conv_layer/2\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        # dense layers (17 * 12 * 50 -> 10200)\n",
        "        self.dense1 = nn.Linear(10200, 2040)\n",
        "        self.dense2 = nn.Linear(2040, 10)\n",
        "\n",
        "        # dropout layers\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.15)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # add sequence of convolutional and max pooling layers\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 10200)\n",
        "        # add dropout layer\n",
        "        x = self.dropout1(x)\n",
        "        # 1st hidden layer, relu activation\n",
        "        x = F.relu(self.dense1(x))\n",
        "        # dropout layer\n",
        "        x = self.dropout2(x)\n",
        "        # 2nd hidden --> output\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "# create a complete CNN\n",
        "cifar_model = cifar_Net()\n",
        "print(cifar_model)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    cifar_model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tN9XuMoOgsd",
        "cellView": "form"
      },
      "source": [
        "print(\"input:\", W, H)\n",
        "\n",
        "(w,h) = output_volume((W,H),(50,50),5,25) #1 \n",
        "print(\"shrink1 out:\",w, h,\"\\tshrink2 in:\",w, h)\n",
        "\n",
        "(w,h) = output_volume((w,h),(30,30),1,3) #1 \n",
        "print(\"shrink2 out:\",w, h,\"\\t1rst in:\",w, h)\n",
        "\n",
        "(w,h) = output_volume((w,h),(11,11),1,1) #1 \n",
        "print(\"1rst out:\",w, h,\"\\t2nd in:\",w//2, h//2)\n",
        "\n",
        "(w,h) = output_volume((w//2,h//2),(7,7),1,1) #2\n",
        "print(\"2nd out:\",w, h,\"\\t3rd in:\",w//2, h//2)\n",
        "\n",
        "(w,h) = output_volume((w//2,h//2),(5,5),1,1) #3\n",
        "print(\"3rd out:\", w, h,\"\\t4th in:\",w//2, h//2)\n",
        "\n",
        "(w,h) = output_volume((w//2,h//2),(3,3),1,1) \n",
        "print(\"4th out:\", w, h,\"\\tfinal out:\",w, h)\n",
        "print(\"flattened:\", w*h)\n",
        "\n",
        "#@title altered CNN architecture\n",
        "class cNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(cNet, self).__init__()\n",
        "\n",
        "        self.shrink1 = nn.Conv2d(3, 3, 50, stride=5, padding=25)\n",
        "        self.shrink2 = nn.Conv2d(3, 3, 30, stride=1, padding=3)\n",
        "\n",
        "        # (138x98x3 image tensor) --> conv1 \n",
        "        # --> 138,98 - 11 + 2 + 1 = 130,90        \n",
        "        self.conv1 = nn.Conv2d(3, 20, 11, padding=1)\n",
        "        #  --> pool /2 --> (65x45x20) --> conv2\n",
        "        self.conv2 = nn.Conv2d(20, 30, 7, padding=1)\n",
        "        # 65,45 - 7 + 2 + 1 = 61,41\n",
        "        # --> pool /2 -->  (30x20x30) --> conv3\n",
        "        self.conv3 = nn.Conv2d(30, 40, 5, padding=1)\n",
        "        # 30,20 - 5 + 2 + 1 = 28,18\n",
        "        # --> pool /2 --> 17,12 (14x9x40) --> conv4\n",
        "        self.conv4 = nn.Conv2d(40, 50, 3, padding=1)\n",
        "        # 14,9 - 3 + 2 + 1 = 14,9\n",
        "        # (14x9x50)=6300 --> flatten\n",
        "\n",
        "        # max pooling --> conv_layer/2\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        # dense layers (14 * 9 * 50 -> 6300)\n",
        "        self.dense1 = nn.Linear(6300, 2100)\n",
        "        self.dense2 = nn.Linear(2100, 10)\n",
        "\n",
        "        # dropout layers\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # add sequence of convolutional and max pooling layers\n",
        "        x = F.relu(self.shrink1(x))\n",
        "        x = F.relu(self.shrink2(x))\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 14*9*50)\n",
        "        # add dropout layer\n",
        "        x = self.dropout1(x)\n",
        "        # 1st hidden layer, relu activation\n",
        "        x = F.relu(self.dense1(x))\n",
        "        # dropout layer\n",
        "        x = self.dropout2(x)\n",
        "        # 2nd hidden --> output\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "# create a complete CNN\n",
        "cnet_model = cNet()\n",
        "print(cnet_model)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    cnet_model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "399KCZHVTJPe"
      },
      "source": [
        "#@title Skynet\n",
        "\n",
        "out = output_volume((H,W),(5,5),2,2)\n",
        "print(\"first in {} \\t first out {}\".format((H,W), out))\n",
        "inn = out\n",
        "out = output_volume(inn,(3,3),2,1)\n",
        "print(\"second in {} \\t second out {}\".format(inn,out))\n",
        "inn = out\n",
        "out = output_volume(inn,(5,5),1,2)\n",
        "print(\"third in {} \\t third out {}\".format(inn,out))\n",
        "inn = (out[0]//2,out[1]//2)\n",
        "out = output_volume(inn,(3,3),1,1)\n",
        "print(\"fourth in {} \\t fourth out {}\".format(inn,out))\n",
        "inn = (out[0]//2,out[1]//2)\n",
        "out = output_volume(inn,(3,3),1,1)\n",
        "print(\"fifth in {} \\t fifth out {}\".format(inn,out))\n",
        "#out0 = out[0]//2\n",
        "#out1 = out[1]//2\n",
        "#print(\"pool in {} \\t pool out {}\".format(out, (out0,out1) ))\n",
        "print(\"\\nflatout = {}\".format(out[0] * out[1]))\n",
        "flatout = out[0] * out[1]\n",
        "\n",
        "class skyNet(nn.Module):\n",
        "    ## TODO: choose an architecture, and complete the class\n",
        "    def __init__(self):\n",
        "        super(skyNet, self).__init__()\n",
        "        ## Define layers of a CNN\n",
        "        # number of nodes in each hidden layer\n",
        "\n",
        "        # input layer ((H , W) -> con1)\n",
        "        self.con1 = nn.Conv2d(3, 12, 5, stride=2, padding=2)\n",
        "        self.con2 = nn.Conv2d(12, 24, 3, stride=2, padding=1)\n",
        "        self.con3 = nn.Conv2d(24, 36, 5, stride=1, padding=2)\n",
        "        self.con4 = nn.Conv2d(36, 36, 3, stride=1, padding=1)\n",
        "        self.con5 = nn.Conv2d(36, 36, 3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.flat = nn.Flatten()\n",
        "        self.dense = nn.Linear(flatout*36, 777)\n",
        "        self.out = nn.Linear(777, 50)\n",
        "        # dropout layers\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ## Define forward pass\n",
        "        # convolutional layers\n",
        "        x = F.relu(self.con1(x))\n",
        "        x = F.relu(self.con2(x))\n",
        "        x = self.pool(F.relu(self.con3(x)))\n",
        "        x = self.pool(F.relu(self.con4(x)))\n",
        "        x = F.relu(self.con5(x))\n",
        "        # flatten input   \n",
        "        #print (x.shape)\n",
        "        x = self.flat(x)\n",
        "        # first dropout layer\n",
        "        x = self.dropout2(x)\n",
        "        # first hidden layer\n",
        "        x = F.relu(self.dense(x))\n",
        "        # second dropout layer\n",
        "        x = self.dropout1(x)\n",
        "        # output layer\n",
        "        x = self.out(x)     \n",
        "        return x\n",
        "\n",
        "#-#-# Do NOT modify the code below this line. #-#-#\n",
        "\n",
        "# instantiate the CNN\n",
        "skynet = skyNet()\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    skynet.cuda()\n",
        "\n",
        "skynet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDYdYLmNuQiQ"
      },
      "source": [
        "for param in skynet.parameters():\n",
        "    print(param.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiLVDXrrKof2"
      },
      "source": [
        "skynet.apply(custom_weight_init)\n",
        "skyopt = torch.optim.SGD(skynet.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "skynet = train(12, loaders_scratch, skynet, skyopt, criterion, use_cuda, 'skynet.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2adMVa0Qx_-"
      },
      "source": [
        "# Sample guess\n",
        "\n",
        "print (data[7].unsqueeze(0).shape, skynet(data[7].unsqueeze(0)).shape)\n",
        "fig = plt.figure(figsize=(16,8),clear=True)\n",
        "axes = fig.subplots(nrows=1, ncols=1, sharex=True, sharey=True)\n",
        "im = data[7].unsqueeze(0)\n",
        "ans = skynet(data[7].unsqueeze(0))\n",
        "\n",
        "im=np.transpose(im.squeeze().detach().cpu().numpy(), (1,2,0))\n",
        "axes.set_title(num2class[np.argmax(ans[0].detach().cpu().numpy())])\n",
        "axes.set_xticks([])\n",
        "axes.set_yticks([])\n",
        "axes.imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oEJvTftjifG"
      },
      "source": [
        "__Question 2:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gffKNXfjifG"
      },
      "source": [
        "__Answer:__  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvR-uWSMjifG"
      },
      "source": [
        "### * * * * *  **TRAIN** here!  * * * * *\n",
        "\n",
        "(IMPLEMENTATION) Implement the Training Algorithm\n",
        "\n",
        "Implement your training algorithm in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at the filepath stored in the variable `save_path`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pwr0U3EjifH"
      },
      "source": [
        "#@ title train def\n",
        "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
        "    \"\"\"returns trained model\"\"\"\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf \n",
        "    #total_loss = 0.0\n",
        "    #total_valid_loss = 0.0\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # initialize variables to monitor training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        #if epoch == 1:\n",
        "        #        print (\"train_loss \\t\\tbatch loss \\t\\trunning avg\")\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        # set the module to training mode\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            ## TODO: find the loss and update the model parameters accordingly\n",
        "            ## record the average training loss, using something like\n",
        "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n",
        "\n",
        "            # initialize optimizer variables to zero\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            output = model(data)\n",
        "            # calculate loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            # update parameters\n",
        "            optimizer.step()\n",
        "            # update training loss\n",
        "            train_loss += loss.item()\n",
        "            #if epoch==1:\n",
        "            #    print (train_loss,\"\\t\", loss.item(),\"\\t\",\n",
        "            #            train_loss/(batch_idx+1))\n",
        "        \n",
        "        train_loss = train_loss/data.size(0)        \n",
        "        #total_loss += train_loss/n_epochs\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            ## TODO: update average validation loss \n",
        "\n",
        "            # forward pass\n",
        "            output = model(data)\n",
        "            # compute batch loss\n",
        "            loss = criterion(output, target)\n",
        "            # update validation loss \n",
        "            valid_loss += loss.item()\n",
        "            #if epoch==1:\n",
        "            #    print (valid_loss,\"\\t\", loss.item(),\"\\t\",\n",
        "            #            valid_loss/(batch_idx+1))\n",
        "        \n",
        "        valid_loss /= data.size(0)\n",
        "        #total_valid_loss += valid_loss/n_epochs\n",
        "        \n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "                                                                    epoch, \n",
        "                                                                    train_loss,\n",
        "                                                                    valid_loss\n",
        "                                                                    ))\n",
        "        #print('\\t\\t\\tTotal Loss: {:.6f} \\tTotal Validation Loss: {:.6f}'.format( \n",
        "        #                                                            total_loss,\n",
        "        #                                                            total_valid_loss))\n",
        "\n",
        "\n",
        "        ## TODO: if the validation loss has decreased, save the model \n",
        "        ##       at the filepath stored in save_path\n",
        "        \n",
        "        if valid_loss < valid_loss_min:\n",
        "            print('Loss decrease ({:.6f} --> {:.6f})'.format(valid_loss_min,\n",
        "                                                                 valid_loss))\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            valid_loss_min = valid_loss\n",
        "        \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylc6-rkRjifI"
      },
      "source": [
        "### (IMPLEMENTATION) Experiment with the Weight Initialization\n",
        "\n",
        "Use the code cell below to define a custom weight initialization, and then train with your weight initialization for a few epochs. Make sure that neither the training loss nor validation loss is `nan`.\n",
        "\n",
        "Later on, you will be able to see how this compares to training with PyTorch's default weight initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "756_-QLzjifI"
      },
      "source": [
        "def custom_weight_init(m):\n",
        "    ## TODO: implement a weight initialization strategy\n",
        "    for p in m.parameters():\n",
        "        n = p.size(0)\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=1.0/(n**0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkuqZv8uoJ3y"
      },
      "source": [
        "#-#-# Do NOT modify the code below this line. #-#-#\n",
        "model_scratch.apply(custom_weight_init)\n",
        "optimizer_scratch = get_optimizer_scratch(model_scratch)\n",
        "model_scratch = train(10, loaders_scratch, model_scratch, optimizer_scratch,\n",
        "                      criterion_scratch, use_cuda, 'ignore_scratch.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8D6dW-fjifJ"
      },
      "source": [
        "### (IMPLEMENTATION) Train and Validate the Model\n",
        "\n",
        "Run the next code cell to train your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQnVkoY4ZfsU"
      },
      "source": [
        "###Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6STU4WvQBFZu"
      },
      "source": [
        "input = torch.randn(33, 3, 600, 800)\n",
        "m = nn.Sequential(\n",
        "            nn.Conv2d(3, 3, 3, stride=3, padding=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(161604, 1000),\n",
        "            nn.Linear(1000, 50)\n",
        ")\n",
        "output = m(input)\n",
        "output.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhhkgrPM9aHQ",
        "cellView": "form"
      },
      "source": [
        "out = output_volume((H,W),(3,3),3,2)\n",
        "print(\"first in {} \\t first out {}\".format((H,W), out))\n",
        "\n",
        "print(\"\\nflattened {}\".format(out[0] * out[1]))\n",
        "flatout = out[0] * out[1]\n",
        "\n",
        "#@title define the mnNet architecture\n",
        "class mnNet(nn.Module):\n",
        "    ## TODO: choose an architecture, and complete the class\n",
        "    def __init__(self):\n",
        "        super(mnNet, self).__init__()\n",
        "        ## Define layers of a CNN\n",
        "        # number of nodes in each hidden layer\n",
        "\n",
        "        # input layer ((H , W) -> nodes_1)\n",
        "        self.con1 = nn.Conv2d(3, 3, 3, stride=3, padding=2)\n",
        "        self.flat = nn.Flatten()\n",
        "        self.den1 = nn.Linear(flatout*3, 1000)\n",
        "        self.out = nn.Linear(1000, 50)\n",
        "        # dropout layers\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ## Define forward pass\n",
        "        # convolutional layers\n",
        "        x = F.relu(self.con1(x))\n",
        "        # flatten input\n",
        "                        #x = x.view(-1, 53868)\n",
        "        x = self.flat(x)\n",
        "        # first dropout layer\n",
        "        x = self.dropout2(x)\n",
        "        # first hidden layer\n",
        "        x = F.relu(self.den1(x))\n",
        "        # second dropout layer\n",
        "        x = self.dropout1(x)\n",
        "        # output layer\n",
        "        x = F.relu(self.out(x))       \n",
        "        return x\n",
        "\n",
        "#-#-# Do NOT modify the code below this line. #-#-#\n",
        "\n",
        "# instantiate the CNN\n",
        "mn = mnNet()\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    mn.cuda()\n",
        "\n",
        "#mn(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FwkRxeWcAsH"
      },
      "source": [
        "mn.apply(custom_weight_init)\n",
        "mnopt= get_optimizer_scratch(mn)\n",
        "epochs = 1 \n",
        "optimizer = mnopt\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "valid_loss_min = np.Inf \n",
        "    \n",
        "train_loss = 0.0\n",
        "valid_loss = 0.0\n",
        "sum_loss = 0.0\n",
        "print (\"sum_loss \\t\\tbatch loss \\t\\trunning avg\")\n",
        "\n",
        "###################\n",
        "# train the model #\n",
        "###################\n",
        "# set the module to training mode\n",
        "mn.train()\n",
        "for batch_idx, (data, target) in enumerate(trainloader):\n",
        "    # move to GPU\n",
        "    if use_cuda:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "    ## TODO: find the loss and update the model parameters accordingly\n",
        "    ## record the average training loss, using something like\n",
        "    ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n",
        "\n",
        "    # initialize optimizer variables to zero\n",
        "    mnopt.zero_grad()\n",
        "    # forward pass\n",
        "    output = mn(data)\n",
        "    #print(\"output shape\",output.shape,\"\\ttarget shape\",target.shape)\n",
        "    # calculate loss\n",
        "    loss = criterion(output, target)\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    # update parameters\n",
        "    mnopt.step()\n",
        "    # update training loss\n",
        "    sum_loss += loss.item()\n",
        "    #train_loss = train_loss + (loss.item() - train_loss)/ (batch_idx + 1))\n",
        "    print (sum_loss,\"\\t\", loss.item(),\"\\t\",\n",
        "           sum_loss/(batch_idx+1))\n",
        "\n",
        "train_loss += sum_loss/data.size(0)\n",
        "print (\"train_loss\",train_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrLeGEhsGvJ6"
      },
      "source": [
        "mn = train(1, loaders_scratch, mn.apply(custom_weight_init), \n",
        "           mnopt, criterion_scratch, use_cuda, 'mn.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shwe3pAlX3eu"
      },
      "source": [
        "######################    \n",
        "# validate the model #\n",
        "######################\n",
        "# set the model to evaluation mode\n",
        "mn.eval()\n",
        "for batch_idx, (data, target) in enumerate(validloader):\n",
        "    # move to GPU\n",
        "    if use_cuda:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "    ## TODO: update average validation loss \n",
        "\n",
        "    # forward pass\n",
        "    output = mn(data)\n",
        "    # compute batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update validation loss \n",
        "    valid_loss += loss.item()\n",
        "    \n",
        "# calculate average losses\n",
        "valid_loss = valid_loss/data.size(0)\n",
        "\n",
        "# print training/validation statistics \n",
        "print('Epoch: {} \\tValidation Loss: {:.6f}'.format(0, valid_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFpMtQxLslbT"
      },
      "source": [
        "#weights = []\n",
        "for param in mn.parameters():\n",
        "    print(param.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUWtxF7TjifK"
      },
      "source": [
        "### (IMPLEMENTATION) Test the Model\n",
        "\n",
        "Run the code cell below to try out your model on the test dataset of landmark images. Run the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 20%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rlT_OnHjifK"
      },
      "source": [
        "def test(loaders, model, criterion, use_cuda):\n",
        "\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "\n",
        "    # set the module to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
        "        # move to GPU\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average test loss \n",
        "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - test_loss))\n",
        "        # convert output probabilities to predicted class\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        # compare predictions to true label\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))\n",
        "\n",
        "# load the model that got the best validation accuracy\n",
        "#model_scratch.load_state_dict(torch.load('model_scratch.pt'))\n",
        "#test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)\n",
        "test(loaders_scratch, mn, criterion_scratch, use_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNv-TSitjifK"
      },
      "source": [
        "---\n",
        "<a id='step2'></a>\n",
        "## Step 2: Create a CNN to Classify Landmarks (using Transfer Learning)\n",
        "\n",
        "You will now use transfer learning to create a CNN that can identify landmarks from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
        "\n",
        "### (IMPLEMENTATION) Specify Data Loaders for the Landmark Dataset\n",
        "\n",
        "Use the code cell below to create three separate [data loaders](http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader): one for training data, one for validation data, and one for test data. Randomly split the images located at `landmark_images/train` to create the train and validation data loaders, and use the images located at `landmark_images/test` to create the test data loader.\n",
        "\n",
        "All three of your data loaders should be accessible via a dictionary named `loaders_transfer`. Your train data loader should be at `loaders_transfer['train']`, your validation data loader should be at `loaders_transfer['valid']`, and your test data loader should be at `loaders_transfer['test']`.\n",
        "\n",
        "If you like, **you are welcome to use the same data loaders from the previous step**, when you created a CNN from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsu_QF_RjifL"
      },
      "source": [
        "### TODO: Write data loaders for training, validation, and test sets\n",
        "## Specify appropriate transforms, and batch_sizes\n",
        "\n",
        "loaders_transfer = loaders_scratch #{'train': None, 'valid': None, 'test': None}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JUFMZEgjifL"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
        "\n",
        "Use the next code cell to specify a [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/stable/optim.html).  Save the chosen loss function as `criterion_transfer`, and fill in the function `get_optimizer_transfer` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXj6mhdAjifL"
      },
      "source": [
        "## TODO: select loss function\n",
        "criterion_transfer = criterion_scratch\n",
        "\n",
        "def get_optimizer_transfer(model):\n",
        "    ## TODO: select and return optimizer\n",
        "    return get_optimizer_scratch(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOSLoA5WjifL"
      },
      "source": [
        "### (IMPLEMENTATION) Model Architecture\n",
        "\n",
        "Use transfer learning to create a CNN to classify images of landmarks.  Use the code cell below, and save your initialized model as the variable `model_transfer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUGEucB4jifL"
      },
      "source": [
        "## TODO: Specify model architecture\n",
        "\n",
        "model_transfer = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#-#-# Do NOT modify the code below this line. #-#-#\n",
        "\n",
        "if use_cuda:\n",
        "    model_transfer = model_transfer.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe5lhT7QjifM"
      },
      "source": [
        "__Question 3:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9nmRLABjifM"
      },
      "source": [
        "__Answer:__  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy-9HzYljifM"
      },
      "source": [
        "### (IMPLEMENTATION) Train and Validate the Model\n",
        "\n",
        "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_transfer.pt'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSTr8ebWjifM"
      },
      "source": [
        "# TODO: train the model and save the best model parameters at filepath 'model_transfer.pt'\n",
        "\n",
        "\n",
        "\n",
        "#-#-# Do NOT modify the code below this line. #-#-#\n",
        "\n",
        "# load the model that got the best validation accuracy\n",
        "model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo5a5extjifN"
      },
      "source": [
        "### (IMPLEMENTATION) Test the Model\n",
        "\n",
        "Try out your model on the test dataset of landmark images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXNojCTpjifN"
      },
      "source": [
        "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SdFMX81jifN"
      },
      "source": [
        "---\n",
        "<a id='step3'></a>\n",
        "## Step 3: Write Your Landmark Prediction Algorithm\n",
        "\n",
        "Great job creating your CNN models! Now that you have put in all the hard work of creating accurate classifiers, let's define some functions to make it easy for others to use your classifiers.\n",
        "\n",
        "### (IMPLEMENTATION) Write Your Algorithm, Part 1\n",
        "\n",
        "Implement the function `predict_landmarks`, which accepts a file path to an image and an integer k, and then predicts the **top k most likely landmarks**. You are **required** to use your transfer learned CNN from Step 2 to predict the landmarks.\n",
        "\n",
        "An example of the expected behavior of `predict_landmarks`:\n",
        "```\n",
        ">>> predicted_landmarks = predict_landmarks('example_image.jpg', 3)\n",
        ">>> print(predicted_landmarks)\n",
        "['Golden Gate Bridge', 'Brooklyn Bridge', 'Sydney Harbour Bridge']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo34Awv3jifN"
      },
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "## the class names can be accessed at the `classes` attribute\n",
        "## of your dataset object (e.g., `train_dataset.classes`)\n",
        "\n",
        "def predict_landmarks(img_path, k):\n",
        "    ## TODO: return the names of the top k landmarks predicted by the transfer learned CNN\n",
        "    \n",
        "\n",
        "\n",
        "# test on a sample image\n",
        "predict_landmarks('images/test/09.Golden_Gate_Bridge/190f3bae17c32c37.jpg', 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlDgEipcjifN"
      },
      "source": [
        "### (IMPLEMENTATION) Write Your Algorithm, Part 2\n",
        "\n",
        "In the code cell below, implement the function `suggest_locations`, which accepts a file path to an image as input, and then displays the image and the **top 3 most likely landmarks** as predicted by `predict_landmarks`.\n",
        "\n",
        "Some sample output for `suggest_locations` is provided below, but feel free to design your own user experience!\n",
        "![](images/sample_landmark_output.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffXAqD11jifO"
      },
      "source": [
        "def suggest_locations(img_path):\n",
        "    # get landmark predictions\n",
        "    predicted_landmarks = predict_landmarks(img_path, 3)\n",
        "    \n",
        "    ## TODO: display image and display landmark predictions\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "# test on a sample image\n",
        "suggest_locations('images/test/09.Golden_Gate_Bridge/190f3bae17c32c37.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTrEVTj9jifO"
      },
      "source": [
        "### (IMPLEMENTATION) Test Your Algorithm\n",
        "\n",
        "Test your algorithm by running the `suggest_locations` function on at least four images on your computer. Feel free to use any images you like.\n",
        "\n",
        "__Question 4:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSQXHwoEjifO"
      },
      "source": [
        "__Answer:__ (Three possible points for improvement)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPMReZ7ZjifO"
      },
      "source": [
        "## TODO: Execute the `suggest_locations` function on\n",
        "## at least 4 images on your computer.\n",
        "## Feel free to use as many code cells as needed.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2nabEkve0VY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y_1Hnaub1ht"
      },
      "source": [
        "# Keras model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3DoMCPOH8ws"
      },
      "source": [
        "## load data and modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC9W_KXqb6gA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import random as rd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy as cat_entropy\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy as sparse_cat_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FqSLXiSd2FR"
      },
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/Udacity/deep_learning/landmark_images.zip').extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHGYTdaL2lMu"
      },
      "source": [
        "try:\n",
        "    train_shapes = pd.read_csv('train_shapes.csv',header=0, index_col=0)\n",
        "    test_shapes = pd.read_csv('test_shapes.csv',header=0, index_col=0)\n",
        "except FileNotFoundError:\n",
        "    print (\"Must upload train_shapes.csv and test_shapes.csv first.\")\n",
        "\n",
        "if False:\n",
        "    \n",
        "    #read train image file data:\n",
        "    train_path = './landmark_images/train/'\n",
        "    cat_path = [train_path + cat + '/' for cat in os.listdir(train_path)]\n",
        "    cat_list = [cat for cat in os.listdir(train_path)]\n",
        "    cats, train_images = [], []\n",
        "    for c in cat_list:\n",
        "        cp = str(train_path + c + '/') \n",
        "        for imfile in os.listdir(cp):\n",
        "            train_images.append(str(cp + imfile))\n",
        "            cats.append(c)\n",
        "\n",
        "    train_image_shapes = []\n",
        "    for imfile in train_images:\n",
        "        train_image_shapes.append(plt.imread(imfile).shape)\n",
        "    heights = [shp[0] for shp in train_image_shapes]    \n",
        "    widths = [shp[1] for shp in train_image_shapes]\n",
        "\n",
        "    train_images_df = pd.DataFrame(columns=['labnum','label',\n",
        "                                            'height','width',\n",
        "                                            'filepath'])\n",
        "    train_images_df['labnum'] = [int(c[:2]) for c in cats]\n",
        "    train_images_df['label'] = [c[3:] for c in cats]\n",
        "    train_images_df['height'] = heights\n",
        "    train_images_df['width'] = widths\n",
        "    train_images_df['filepath'] = train_images\n",
        "\n",
        "    #read test image file data:\n",
        "    test_path = './landmark_images/test/'\n",
        "    cat_path = [test_path + cat + '/' for cat in os.listdir(test_path)]\n",
        "    cat_list = [cat for cat in os.listdir(test_path)]\n",
        "    cats, test_images = [], []\n",
        "    for c in cat_list:\n",
        "        cp = str(test_path + c + '/') \n",
        "        for imfile in os.listdir(cp):\n",
        "            test_images.append(str(cp + imfile))\n",
        "            cats.append(c)\n",
        "\n",
        "    test_image_shapes = []\n",
        "    for imfile in test_images:\n",
        "        test_image_shapes.append(plt.imread(imfile).shape)\n",
        "    heights = [shp[0] for shp in test_image_shapes]    \n",
        "    widths = [shp[1] for shp in test_image_shapes]\n",
        "\n",
        "    test_images_df = pd.DataFrame(columns=['labnum','label',\n",
        "                                            'height','width',\n",
        "                                            'filepath'])\n",
        "    test_images_df['labnum'] = [int(c[:2]) for c in cats]\n",
        "    test_images_df['label'] = [c[3:] for c in cats]\n",
        "    test_images_df['height'] = heights\n",
        "    test_images_df['width'] = widths\n",
        "    test_images_df['filepath'] = test_images\n",
        "\n",
        "    #save dfs\n",
        "    train_images_df.to_csv('train_shapes.csv')\n",
        "    test_images_df.to_csv('test_shapes.csv')\n",
        "\n",
        "    #cleanup\n",
        "    del train_images, train_image_shapes, train_path\n",
        "    del test_images, test_image_shapes, test_path\n",
        "    del cats, cat_list, cat_path, widths, heights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jzYMZxAHmeA"
      },
      "source": [
        "## Only want exaclty 600x800 images\n",
        "testable_df = test_shapes.loc[lambda df: df['width'] == 800].loc[lambda df: df['height'] == 600]\n",
        "trainable_df = train_shapes.loc[lambda df: df['width'] == 800].loc[lambda df: df['height'] == 600]\n",
        "\n",
        "## Make class dict\n",
        "classvals = list(set([(str(pair[0]),pair[1]) for pair in test_shapes[['labnum','label']].values]))\n",
        "num2class = {int(x):y for x,y in classvals}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhOtmPYjJXHr"
      },
      "source": [
        "f = plt.figure(num=1, figsize=(20,20))\n",
        "for i in range(3):\n",
        "    randix = rd.choice(list(testable_df.index))\n",
        "    img = plt.imread(test_shapes.iloc[randix]['filepath'])/255.\n",
        "    tar = test_shapes.iloc[randix]['labnum']\n",
        "\n",
        "    ax = f.add_subplot(1,3,i+1)\n",
        "    ax.set_title(\"Class #\"+str(tar)+\" \"+num2class[tar])\n",
        "    ax.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fic3YAARH11N"
      },
      "source": [
        "## experimental models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5MXNYt2H1du",
        "cellView": "form"
      },
      "source": [
        "#@title Experimental Keras Model\n",
        "\n",
        "input_layer = layers.Input(shape=(600,800,3))\n",
        "z = layers.Conv2D(10, 3, \n",
        "                  strides=2,\n",
        "                  activation='relu',\n",
        "                  #kernel_initializer='uniform',\n",
        "                  use_bias=True,\n",
        "                  padding='same')(input_layer)\n",
        "\n",
        "y = layers.Conv2D(10, 3, \n",
        "                  strides=2,\n",
        "                  activation='relu',\n",
        "                  #kernel_initializer='normal',\n",
        "                  use_bias=True,\n",
        "                  padding='same')(z)\n",
        "\n",
        "x = layers.Conv2D(20, 7, \n",
        "                  strides=1,\n",
        "                  activation='relu',\n",
        "                  use_bias=True,\n",
        "                  padding='valid')(y)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Conv2D(30, 5, \n",
        "                  strides=1,\n",
        "                  activation='relu',\n",
        "                  use_bias=True,\n",
        "                  padding='valid')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Conv2D(40, 3, \n",
        "                  strides=1,\n",
        "                  activation='relu',\n",
        "                  use_bias=True,\n",
        "                  padding='valid')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Conv2D(60, 3, \n",
        "                  strides=1,\n",
        "                  activation='relu',\n",
        "                  use_bias=True,\n",
        "                  padding='valid')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(1000, activation='relu')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "output_layer = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "shrinkX = Model(input_layer, y)\n",
        "modelX = Model(input_layer, output_layer)\n",
        "modelX.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhnyesCBnQFP"
      },
      "source": [
        "def scale(a):\n",
        "    return (a-a.min())/(a.max()-a.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5VL_7C3hPQf"
      },
      "source": [
        "f = plt.figure(num=2,figsize=(20,5))\n",
        "sp1 = f.add_subplot(1,4,1)\n",
        "sp2 = f.add_subplot(1,4,2)\n",
        "sp3 = f.add_subplot(1,4,3)\n",
        "sp4 = f.add_subplot(1,4,4)\n",
        "\n",
        "randix = rd.choice(list(trainable_df.index))\n",
        "img = plt.imread(train_shapes.iloc[randix]['filepath'])/255.\n",
        "tar = train_shapes.iloc[randix]['labnum']\n",
        "\n",
        "out_img = shrinkX(np.expand_dims(img,0))\n",
        "out1 = out_img.numpy()[0,:,:,0:3]\n",
        "out2 = out_img.numpy()[0,:,:,5:8]\n",
        "out3 = out_img.numpy()[0,:,:,7:10]\n",
        "\n",
        "\n",
        "sp1.set_title(\"Class #\"+str(tar)+\" \"+num2class[tar])\n",
        "sp1.imshow(scale(img))\n",
        "sp2.set_title(\"Model Output One\")\n",
        "sp2.imshow(scale(out1))\n",
        "sp3.set_title(\"Model Output Two\")\n",
        "sp3.imshow(scale(out2))\n",
        "sp4.set_title(\"Model Output Three\")\n",
        "sp4.imshow(scale(out3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO5gf2WfmEBE"
      },
      "source": [
        "print(\"min\\t max\\t avg\\t std\")\n",
        "for i in range(10):  \n",
        "    print(\"{:.4f}\\t {:.4f}\\t {:.4f}\\t {:.4f}\".format(   out_img.numpy()[0,:,:,i].min(), \n",
        "                                                        out_img.numpy()[0,:,:,i].max(),\n",
        "                                                        out_img.numpy()[0,:,:,i].mean(),\n",
        "                                                        out_img.numpy()[0,:,:,i].std()) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1-pTNYE-h3f"
      },
      "source": [
        "#### build and train fnxs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkknoBobJ4R5"
      },
      "source": [
        "#traintupleDG, testupleDG = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zo9wjSTBtC7"
      },
      "source": [
        "## Initiate hyperparameters\n",
        "epochs = 5\n",
        "learning_rate = 0.01\n",
        "batch_size = 40\n",
        "validation_batch_size = 20\n",
        "\n",
        "## Construct datasets\n",
        "x_train = np.array([plt.imread(fp) for fp in trainable_df[\"filepath\"]])/255.\n",
        "y_train = float(trainable_df[[\"labnum\"]].values.squeeze())\n",
        "\n",
        "x_test = np.array([plt.imread(fp) for fp in testable_df[\"filepath\"]])/255.\n",
        "y_test = float(testable_df[[\"labnum\"]].values.squeeze())\n",
        "\n",
        "traintuple = (x_train[:4], y_train[:4])\n",
        "testtuple = (x_test[:4], y_test[:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cGHd2HVb1MD"
      },
      "source": [
        "del x_test\n",
        "del y_test\n",
        "del x_train\n",
        "del y_train\n",
        "del traintuple\n",
        "del testtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtngjIXgUvws",
        "cellView": "form"
      },
      "source": [
        "#@title Define functions for building, training, compiling and evaluating models\n",
        "\n",
        "def build_model(verbose=False):\n",
        "    '''Builds a model and returns it uncompiled\n",
        "\n",
        "    '''\n",
        "    input_layer = layers.Input(shape=(600,800,3))\n",
        "    z = layers.Conv2D(10, 3, \n",
        "                    strides=2,\n",
        "                    activation='relu',\n",
        "                    #kernel_initializer='uniform',\n",
        "                    use_bias=True,\n",
        "                    padding='same')(input_layer)\n",
        "\n",
        "    y = layers.Conv2D(10, 3, \n",
        "                    strides=2,\n",
        "                    activation='relu',\n",
        "                    #kernel_initializer='normal',\n",
        "                    use_bias=True,\n",
        "                    padding='same')(z)\n",
        "\n",
        "    x = layers.Conv2D(20, 7, \n",
        "                    strides=1,\n",
        "                    activation='relu',\n",
        "                    use_bias=True,\n",
        "                    padding='valid')(y)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = layers.Conv2D(30, 5, \n",
        "                    strides=1,\n",
        "                    activation='relu',\n",
        "                    use_bias=True,\n",
        "                    padding='valid')(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = layers.Conv2D(40, 3, \n",
        "                    strides=1,\n",
        "                    activation='relu',\n",
        "                    use_bias=True,\n",
        "                    padding='valid')(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = layers.Conv2D(60, 3, \n",
        "                    strides=1,\n",
        "                    activation='relu',\n",
        "                    use_bias=True,\n",
        "                    padding='valid')(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(1000, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    output_layer = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "    return Model(input_layer, output_layer)\n",
        "\n",
        "## Same training loop for all models\n",
        "def train(model, traintuple, valtuple, epochs=epochs):\n",
        "    '''Train a model on the given sets of data\n",
        "        Params: the given model,\n",
        "                the train data as a tuple of x,y,\n",
        "                the test data as a tuple of x,y\n",
        "        Returns: a dictionary of metric values after each epoch of training\n",
        "    '''\n",
        "    (x_train, y_train) = traintuple    \n",
        "    history = model.fit(x=x_train,\n",
        "                        y=y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=valtuple,\n",
        "                        validation_batch_size=validation_batch_size,\n",
        "                        epochs=epochs,  \n",
        "                        verbose=1)   \n",
        "    return history.history\n",
        "print(\"Loaded function train(model, traintuple, testuple, epochs=epochs)\")\n",
        "\n",
        "## All models are compiled the same\n",
        "def compile_model(model):    \n",
        "    model.compile(  loss=sparse_cat_entropy,\n",
        "                    optimizer=SGD(learning_rate=learning_rate),                    \n",
        "                    metrics=['acc'])\n",
        "    print (\"Compiled model\", model.name)\n",
        "print(\"loaded function compile_model(model)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7okEhFBLQWit"
      },
      "source": [
        "def build_mini(verbose=False):\n",
        "    '''Builds a minimal experimental model and returns it uncompiled\n",
        "\n",
        "    '''\n",
        "    input_layer = layers.Input(shape=(600,800,3))\n",
        "    x = layers.Conv2D(10, 3, \n",
        "                    strides=2,\n",
        "                    activation='relu',\n",
        "                    use_bias=True,\n",
        "                    padding='same')(input_layer)\n",
        "\n",
        "    x = layers.MaxPooling2D(3)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    output_layer = layers.Dense(50, activation='softmax')(x)\n",
        "\n",
        "    return Model(input_layer, output_layer, name='minimodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaAnv1W3RVCW"
      },
      "source": [
        "def qtrain(model, traintuple, valtuple, epochs=epochs):\n",
        "    '''Train a model on the given sets of data\n",
        "        Params: the given model,\n",
        "                the train data as a tuple of x,y,\n",
        "                the test data as a tuple of x,y\n",
        "        Returns: a dictionary of metric values after each epoch of training\n",
        "    '''\n",
        "    (x_train, y_train) = traintuple    \n",
        "    history = model.fit(x=x_train,\n",
        "                        y=y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=valtuple,\n",
        "                        validation_batch_size=validation_batch_size,\n",
        "                        epochs=epochs,  \n",
        "                        verbose=1)   \n",
        "    return history.history\n",
        "\n",
        "def qcompile_model(model):    \n",
        "    model.compile(  loss=sparse_cat_entropy,\n",
        "                    optimizer=SGD(learning_rate=learning_rate))#,                    \n",
        "                    #metrics=['acc'])\n",
        "    print (\"Compiled model\", model.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wFhydCbIgOV"
      },
      "source": [
        "epochs = 1\n",
        "learning_rate = 0.01\n",
        "batch_size = 2\n",
        "validation_batch_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP2VqeELLpGX"
      },
      "source": [
        "### Build\n",
        "mm = build_mini()\n",
        "\n",
        "### Compile\n",
        "qcompile_model(mm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PoiiiIgJFiT"
      },
      "source": [
        "######### Print out summary table\n",
        "print(mm.summary(),\"\\n\")\n",
        "\n",
        "######## Plot model diagrams\n",
        "tf.keras.utils.plot_model(mm, \n",
        "                          show_layer_names=True, \n",
        "                          show_shapes=True, \n",
        "                          to_file=\"mm.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6XfSpmr6X2n"
      },
      "source": [
        "### Train\n",
        "train_stats = qtrain(mm, traintuple, testtuple, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0p3SmHkY-We"
      },
      "source": [
        "randix = rd.choice(list(trainable_df.index))\n",
        "img = plt.imread(train_shapes.iloc[randix]['filepath'])/255.\n",
        "tar = train_shapes.iloc[randix]['labnum']\n",
        "\n",
        "out = np.argmax(mm(np.expand_dims(img,0)).numpy().squeeze())\n",
        "print(\"target\",tar,\"\\toutput\", out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GMD2GO0awLv"
      },
      "source": [
        "traintuple[1].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqgHU311wzMH"
      },
      "source": [
        "#pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXF7QM5DqrdR"
      },
      "source": [
        "---\n",
        "## Define the Model\n",
        "\n",
        "To define a model for training we'll follow these steps:\n",
        "1. Load in a pre-trained VGG16 model\n",
        "2. \"Freeze\" all the parameters, so the net acts as a fixed feature extractor \n",
        "3. Remove the last layer\n",
        "4. Replace the last layer with a linear classifier of our own\n",
        "\n",
        "**Freezing simply means that the parameters in the pre-trained model will *not* change during training.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7uYVJSvRqrdS"
      },
      "source": [
        "# Load the pretrained model from pytorch\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "# print out the model structure\n",
        "print(vgg16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh9-blyCy63z"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "(classifier): Sequential(\n",
        "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
        "    (1): ReLU(inplace=True)\n",
        "    (2): Dropout(p=0.5, inplace=False)\n",
        "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
        "    (4): ReLU(inplace=True)\n",
        "    (5): Dropout(p=0.5, inplace=False)\n",
        "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
        "  )\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aJ_miYhyqrdT"
      },
      "source": [
        "# Freeze training for all \"features\" layers\n",
        "for param in vgg16.features.parameters():\n",
        "    #param.requires_grad = False\n",
        "    print(param.size(), param.requires_grad) \n",
        "    \n",
        "for clsfr in vgg16.classifier[:2]:\n",
        "    for prmtr in clsfr.parameters():\n",
        "        #prmtr.requires_grad = False\n",
        "        print(prmtr.size(), prmtr.requires_grad) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJiMZPPaqrdT"
      },
      "source": [
        "---\n",
        "### Final Classifier Layer\n",
        "\n",
        "Once you have the pre-trained feature extractor, you just need to modify and/or add to the final, fully-connected classifier layers. In this case, we suggest that you repace the last layer in the vgg classifier group of layers. \n",
        "> This layer should see as input the number of features produced by the portion of the network that you are not changing, and produce an appropriate number of outputs for the flower classification task.\n",
        "\n",
        "You can access any layer in a pretrained network by name and (sometimes) number, i.e. `vgg16.classifier[6]` is the sixth layer in a group of layers named \"classifier\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoVOog3AqrdU"
      },
      "source": [
        "n_inputs = vgg16.classifier[6].in_features\n",
        "\n",
        "# new layers automatically have requires_grad = True\n",
        "fork_layer = nn.Linear(n_inputs, len(classes))\n",
        "#last_layer = nn.Linear(50*len(classes), len(classes))\n",
        "\n",
        "vgg16.classifier[6] = fork_layer\n",
        "#vgg16.classifier[7] = last_layer\n",
        "\n",
        "# if GPU is available, move the model to GPU\n",
        "if use_cuda:\n",
        "    vgg16.cuda()\n",
        "\n",
        "# check to see that your last layer produces the expected number of outputs\n",
        "print(vgg16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3J_wxZ-_D2o"
      },
      "source": [
        "# Check Freeze training for all layers\n",
        "for param in vgg16.features.parameters():\n",
        "    print(param.size(), param.requires_grad) \n",
        "for param in vgg16.classifier.parameters():\n",
        "    print(param.size(), param.requires_grad) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "61rcpZhiqrdU"
      },
      "source": [
        "### Specify [Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
        "\n",
        "Below we'll use cross-entropy loss and stochastic gradient descent with a small learning rate. Note that the optimizer accepts as input _only_ the trainable parameters `vgg.classifier.parameters()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "QgFElRorqrdU"
      },
      "source": [
        "#import torch.optim as optim\n",
        "\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer (stochastic gradient descent) and learning rate = 0.001\n",
        "optimizer = optim.SGD(vgg16.classifier.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1ehkG7gqrdU"
      },
      "source": [
        "---\n",
        "## Training\n",
        "\n",
        "Here, we'll train the network.\n",
        "\n",
        "> **Exercise:** So far we've been providing the training code for you. Here, I'm going to give you a bit more of a challenge and have you write the code to train the network. Of course, you'll be able to see my solution if you need help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GooOaIDnEzit"
      },
      "source": [
        "optimizer = optim.SGD(vgg16.classifier.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1Ra8Ef7gqrdV"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 2\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    # model by default is set to train\n",
        "    for batch_i, (data, target) in enumerate(trainloader):\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = vgg16(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss \n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        if batch_i % 20 == 19:    # print training loss every specified number of mini-batches\n",
        "            print('Epoch %d, Batch %d loss: %.16f' %\n",
        "                  (epoch, batch_i + 1, train_loss / 20))\n",
        "            train_loss = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1-0jgGFqrdV"
      },
      "source": [
        "---\n",
        "## Testing\n",
        "\n",
        "Below you see the test accuracy for each flower class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wiWT9uPJo_l"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(test_data, \n",
        "                                         batch_size=30,\n",
        "                                         sampler=test_sampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb0vVdzlqrdW"
      },
      "source": [
        "# track test loss \n",
        "# over 5 flower classes\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(50))\n",
        "class_total = list(0. for i in range(50))\n",
        "\n",
        "vgg16.eval() # eval mode\n",
        "\n",
        "# iterate over test data\n",
        "for data, target in testloader:\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if use_cuda:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = vgg16(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update  test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not use_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(data.size(0)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate avg test loss\n",
        "test_loss = test_loss/len(testloader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(50):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            num2class[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (num2class[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Dc2wopcTqrdW"
      },
      "source": [
        "### Visualize Sample Test Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCLnN-q1qrdX"
      },
      "source": [
        "# obtain one batch of test images\n",
        "\n",
        "try:\n",
        "    images, labels = dataiter.next()\n",
        "except NameError:\n",
        "    dataiter = iter(testloader)\n",
        "    images, labels = dataiter.next()\n",
        "images.numpy()\n",
        "\n",
        "# move model inputs to cuda, if GPU available\n",
        "if use_cuda:\n",
        "    images = images.cuda()\n",
        "\n",
        "# get sample outputs\n",
        "output = vgg16(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max( output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not use_cuda else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(21, 7.5))\n",
        "for idx in np.arange(8):\n",
        "    ax = fig.add_subplot(2, 8/2, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(np.transpose(images.cpu()[idx], (1, 2, 0)))\n",
        "    ax.set_title(\"{}\\n({})\".format(num2class[preds[idx]], num2class[labels[idx].item()]),\n",
        "                                  color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK43WTmHOJfj"
      },
      "source": [
        "preds[idx], labels[idx].item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuiIJ335OfQX"
      },
      "source": [
        "num2class[preds[idx]], labels[idx].item()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S1H7jclw1Iw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkQqtvRoRVD3"
      },
      "source": [
        ""
      ]
    }
  ]
}