{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "landmark draft.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kvR-uWSMjifG"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT6Wia07jie5"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "## Project: Write an Algorithm for Landmark Classification\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
        "\n",
        "> **Note**: Once you have completed all the code implementations, you need to finalize your work by exporting the Jupyter Notebook as an HTML document. Before exporting the notebook to HTML, all the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to **File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
        "\n",
        "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
        "\n",
        ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
        "\n",
        "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this Jupyter notebook.\n",
        "\n",
        "---\n",
        "### Why We're Here\n",
        "\n",
        "Photo sharing and photo storage services like to have location data for each photo that is uploaded. With the location data, these services can build advanced features, such as automatic suggestion of relevant tags or automatic photo organization, which help provide a compelling user experience. Although a photo's location can often be obtained by looking at the photo's metadata, many photos uploaded to these services will not have location metadata available. This can happen when, for example, the camera capturing the picture does not have GPS or if a photo's metadata is scrubbed due to privacy concerns.\n",
        "\n",
        "If no location metadata for an image is available, one way to infer the location is to detect and classify a discernible landmark in the image. Given the large number of landmarks across the world and the immense volume of images that are uploaded to photo sharing services, using human judgement to classify these landmarks would not be feasible.\n",
        "\n",
        "In this notebook, you will take the first steps towards addressing this problem by building models to automatically predict the location of the image based on any landmarks depicted in the image. At the end of this project, your code will accept any user-supplied image as input and suggest the top k most relevant landmarks from 50 possible landmarks from across the world. The image below displays a potential sample output of your finished project.\n",
        "\n",
        "![Sample landmark classification output](images/sample_landmark_output.png)\n",
        "\n",
        "\n",
        "# The Road Ahead\n",
        "\n",
        "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
        "\n",
        "* [Step 0](#step0): Download Datasets and Install Python Modules\n",
        "* [Step 1](#step1): Create a CNN to Classify Landmarks (from Scratch)\n",
        "* [Step 2](#step2): Create a CNN to Classify Landmarks (using Transfer Learning)\n",
        "* [Step 3](#step3): Write Your Landmark Prediction Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWwhrq3ue6Dy"
      },
      "source": [
        "<a id='step0'></a>\n",
        "## Step 0: Download Datasets and Install Python Modules\n",
        "\n",
        "**Note: if you are using the Udacity workspace, *YOU CAN SKIP THIS STEP*. The dataset can be found in the `/data` folder and all required Python modules have been installed in the workspace.**\n",
        "\n",
        "Download the [landmark dataset](https://udacity-dlnfd.s3-us-west-1.amazonaws.com/datasets/landmark_images.zip).\n",
        "Unzip the folder and place it in this project's home directory, at the location `/landmark_images`. The landmark images are a subset of the **Google Landmarks Dataset v2**.\n",
        "\n",
        "Install the following Python modules:\n",
        "* cv2\n",
        "* matplotlib\n",
        "* numpy\n",
        "* PIL\n",
        "* torch\n",
        "* torchvision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emIdRzaxlEB7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler\n",
        "\n",
        "from PIL import Image\n",
        "import random as rd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30UiK0bgaur4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/google')\n",
        "\n",
        "import zipfile\n",
        "zipfile.ZipFile('/content/google/MyDrive/Data/landmark_images.zip').extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGVjJnF-WQuy"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "    print(torch.cuda.list_gpu_processes(), torch.cuda.memory_snapshot())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuloVeLD6fvL"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HGgiGGqjie_"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Data Loaders for the Landmark Dataset\n",
        "\n",
        "Use the code cell below to create three separate [data loaders](http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader): one for training data, one for validation data, and one for test data. Randomly split the images located at `landmark_images/train` to create the train and validation data loaders, and use the images located at `landmark_images/test` to create the test data loader.\n",
        "\n",
        "All three of your data loaders should be accessible via a dictionary named `loaders_scratch`. Your train data loader should be at `loaders_scratch['train']`, your validation data loader should be at `loaders_scratch['valid']`, and your test data loader should be at `loaders_scratch['test']`.\n",
        "\n",
        "You may find [this documentation on custom datasets](https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder) to be a useful resource.  If you are interested in augmenting your training and/or validation data, check out the wide variety of [transforms](http://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transform)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mgOhgXEB2S1"
      },
      "source": [
        "# Define transforms for the training data and testing data  \n",
        "\n",
        "# image directory location:\n",
        "data_dir = './landmark_images'\n",
        "\n",
        "# set final target height H and width W for all images\n",
        "H = 224\n",
        "W = 224\n",
        "# intermediate size values for train data augmentations\n",
        "h1 = 248\n",
        "w1 = 248\n",
        "h0 = 272\n",
        "w0 = 272\n",
        "\n",
        "train_transforms = transforms.Compose([ #transforms.Pad((0,0,200,0), padding_mode='reflect'),\n",
        "                                        transforms.Resize((h0, w0)),\n",
        "                                        #transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.RandomResizedCrop((H,W)),\n",
        "                                        #transforms.RandomRotation(10, expand=True, fill=0),\n",
        "                                        #transforms.CenterCrop((H, W)),\n",
        "                                        transforms.ToTensor()]) \n",
        "\n",
        "test_transforms = transforms.Compose([ transforms.Resize((H, W)),\n",
        "                                       transforms.ToTensor()])\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
        "\n",
        "# Toggle taking validation data from either train_data or test_data\n",
        "data = train_data\n",
        "#data = test_data\n",
        "\n",
        "# Randomly separate out validation data\n",
        "datasize = len(data) \n",
        "indices = list(range(datasize))\n",
        "np.random.shuffle(indices)\n",
        "split = int(datasize//6)\n",
        "data_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# cumbersome implementation of choice for which data set to use\n",
        "if data is train_data:\n",
        "    train_idx = data_idx\n",
        "else:\n",
        "    train_idx = list(range(len(train_data)))\n",
        "\n",
        "# define samplers for obtaining training and validation batches    \n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "\n",
        "num_workers=2 # not sure what difference this may make\n",
        "\n",
        "## Write data loaders for training, validation, and test sets\n",
        "## Specify appropriate transforms, and batch_sizes\n",
        "\n",
        "## low batch sizes to avoid crashing the GPU!\n",
        "trainloader = torch.utils.data.DataLoader(train_data, \n",
        "                                          batch_size=17,\n",
        "                                          sampler=train_sampler,\n",
        "                                          num_workers=num_workers)\n",
        "\n",
        "validloader = torch.utils.data.DataLoader(data, \n",
        "                                          batch_size=16,\n",
        "                                          sampler=valid_sampler,\n",
        "                                          num_workers=num_workers)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_data, \n",
        "                                         batch_size=10,\n",
        "                                         sampler=test_sampler,\n",
        "                                         num_workers=num_workers)\n",
        "\n",
        "loaders = {'train': trainloader, 'valid': validloader, 'test': testloader}\n",
        "loaders_scratch = loaders\n",
        "loaders_transfer = loaders\n",
        "\n",
        "## Define class dicts\n",
        "classes = test_data.classes\n",
        "classes.sort()\n",
        "class2num = {c[3:]:int(c[:2]) for c in classes}\n",
        "num2class = {int(c[:2]):c[3:] for c in classes}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlYEy0mAxG6U"
      },
      "source": [
        "for i in range(5,36):\n",
        "    print(i, 4164 // i, 4164 % i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqLqEaFB7RWW"
      },
      "source": [
        "for i in range(5,37):\n",
        "    print(i, 832 // i, 832 % i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44DffOjKjifB"
      },
      "source": [
        "**Question 1:** Describe your chosen procedure for preprocessing the data. \n",
        "- How does your code resize the images (by cropping, stretching, etc)?  What size did you pick for the input tensor, and why?\n",
        "- Did you decide to augment the dataset?  If so, how (through translations, flips, rotations, etc)?  If not, why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wRtIsOHjifC"
      },
      "source": [
        "**Answer**: \n",
        "\n",
        "\n",
        "\n",
        "I also wanted at first was keep size as large as possible: a minimum of 300x400 pixels. This was likely to have been one of the reasons I was having so much trouble with the GPU running out of memory and otherwise crashing. It took me a while to realize that. Eventually I settled on 224x224 because that is the size expected by torchvision's pre-trained models. I could probably get it lower than that but with all the other variables at play I decided to just keep to what works.\n",
        "\n",
        "Given a relatively small data set to work with, the training images were \"augmented\" by making a few random distorions to them.\n",
        "\n",
        "\n",
        "__Answer:__ \n",
        "\n",
        "I experimented with a variety of shapes and sizes and, for the training-set images, with different augmentations such as random rotation, horizontal flipping, random cropping, etc.  The first thing I found out is that if the images are too large that will make the GPU prone to crash from lack of memory!  (And it took me a while to figure out what was causing that. I also decreased the batch-size, and the GPU is not crashing anymore).  \n",
        "\n",
        "The second thing I realized was that all the augmentations were being applied at run-time to each single image, which slowed down the training process considerably. I was also concerned that too much distortion might make accurate categorization more difficult. I suppose there is a certain limit to how much randomicity can be applied before all the images look essentially the same, but before that at some point a maximization of generality without too much loss of accuracy.  I did not find that point though, and in the end I kept only the augmentations that seemed to take the least time to process.\n",
        "\n",
        "The specific size of 224x224 I chose because that is the expected input size for most of the pretrained models available from torchvision.  It could probably be lower than that but with all the other variables at play I decided to just keep to what works. That size doesn't seem to crash the GPU, so I left it at that!\n",
        "\n",
        "I wanted to find a way to keep the 3:4 aspect ratio so as to minimize distortion for the apparently most-common 600x800 images. In the end I decided that the best way to minimize the distortion across *all* the different aspect ratios was to reduce every image down to a 1:1 \"fair-and-square\" aspect ratio.\n",
        "\n",
        "The test images were not distorted beyond being resized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEcUnrYCjifC"
      },
      "source": [
        "### (IMPLEMENTATION) Visualize a Batch of Training Data\n",
        "\n",
        "Use the code cell below to retrieve a batch of images from your train data loader, display at least 5 images simultaneously, and label each displayed image with its class name (e.g., \"Golden Gate Bridge\").\n",
        "\n",
        "Visualizing the output of your data loader is a great way to ensure that your data loading and preprocessing are working as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWrT07-4jifD"
      },
      "source": [
        "## visualize a batch of the train loader\n",
        "\n",
        "#del train_data_iter\n",
        "try:\n",
        "    images, labels = train_data_iter.next()\n",
        "except NameError:\n",
        "    train_data_iter = iter(loaders_scratch['train'])\n",
        "    images, labels = train_data_iter.next()\n",
        "\n",
        "fig = plt.figure(figsize=(17,8),clear=True)\n",
        "axes = fig.subplots(nrows=2, ncols=4, sharex=True, sharey=True)\n",
        "axes = axes.flatten()\n",
        "for i, im in enumerate(images[0:8]):\n",
        "    ax = axes[i]\n",
        "    title = num2class[labels[i].item()]\n",
        "    ax.set_title(title)\n",
        "    axes[i].set_xticks([])\n",
        "    axes[i].set_yticks([])\n",
        "    ax.imshow(np.transpose(im, (1,2,0)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYBJPQbe4qr2"
      },
      "source": [
        "### Preview test images\n",
        "\n",
        "#del test_data_iter\n",
        "try:\n",
        "    images, labels = test_data_iter.next()\n",
        "except NameError:\n",
        "    test_data_iter = iter(testloader)\n",
        "    images, labels = test_data_iter.next()\n",
        "\n",
        "labels = labels.tolist()\n",
        "\n",
        "fig = plt.figure(figsize=(18,8.5))\n",
        "axes = fig.subplots(nrows=2, ncols=4)\n",
        "axes = axes.flatten()\n",
        "\n",
        "idx = rd.randrange(images.shape[0]-8)\n",
        "for i in range(idx,idx+8):\n",
        "    img = np.transpose(images[i], (1,2,0))\n",
        "    title = num2class[labels[i]]\n",
        "    axes[i-idx].set_title(title)\n",
        "    axes[i-idx].set_xticks([])\n",
        "    axes[i-idx].set_yticks([])\n",
        "    axes[i-idx].imshow(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVcaiWXqDIyS"
      },
      "source": [
        "img.min(), img.max(), img.mean(), img.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITQnn5K5jie-"
      },
      "source": [
        "<a id='step1'></a>\n",
        "## Step 1: Create a CNN to Classify Landmarks (from Scratch)\n",
        "\n",
        "In this step, you will create a CNN that classifies landmarks.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 20%.\n",
        "\n",
        "Although 20% may seem low at first glance, it seems more reasonable after realizing how difficult of a problem this is. Many times, an image that is taken at a landmark captures a fairly mundane image of an animal or plant, like in the following picture.\n",
        "\n",
        "<img src=\"images/train/00.Haleakala_National_Park/084c2aa50d0a9249.jpg\" alt=\"Bird in Haleakalā National Park\" style=\"width: 400px;\"/>\n",
        "\n",
        "Just by looking at that image alone, would you have been able to guess that it was taken at the Haleakalā National Park in Hawaii?\n",
        "\n",
        "An accuracy of 20% is significantly better than random guessing, which would provide an accuracy of just 2%. In Step 2 of this notebook, you will have the opportunity to greatly improve accuracy by using transfer learning to create a CNN.\n",
        "\n",
        "Remember that practice is far ahead of theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_94OXey5jifE"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
        "\n",
        "Use the next code cell to specify a [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/stable/optim.html).  Save the chosen loss function as `criterion_scratch`, and fill in the function `get_optimizer_scratch` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV9O3s9QjifF"
      },
      "source": [
        "## select scratch loss function\n",
        "criterion_scratch = nn.CrossEntropyLoss()\n",
        "\n",
        "def get_optimizer_scratch(model, lr=0.001):\n",
        "    ## select and return an optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3pBgngajifF"
      },
      "source": [
        "### (IMPLEMENTATION) Model Architecture\n",
        "\n",
        "Create a CNN to classify images of landmarks.  Use the template in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia2gNkXV10Rl"
      },
      "source": [
        "# Calculate a layer's output shape\n",
        "def output_volume(w, f, s, p):\n",
        "    ''' Calculate a convolutional layer's output size\n",
        "    params:\n",
        "    w = input volume (tuple)\n",
        "    f = kernel size (tuple)\n",
        "    s = stride (int)\n",
        "    p = padding (int)\n",
        "    '''\n",
        "    x = (w[0] - f[0] + 2*p)/s + 1\n",
        "    y = (w[1] - f[1] + 2*p)/s + 1\n",
        "    return ( int(np.floor(x)), int(np.floor(y)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPBTFE2l-ABL"
      },
      "source": [
        "#@ title scratchNet\n",
        "\n",
        "# First calculate output volume of convolutional layers\n",
        "out = output_volume((H,W),(3,3),1,1)\n",
        "print(\"first in {} \\t first out {}\".format((H,W), out))\n",
        "inn = out\n",
        "out = output_volume(inn,(3,3),1,1)\n",
        "print(\"second in {} \\t second out {} >> pool12\".format(inn,out))\n",
        "inn = (out[0]//2, out[1]//2)\n",
        "out = output_volume(inn,(3,3),1,1)\n",
        "print(\"third in {} \\t third out {}\".format(inn,out))\n",
        "inn = out\n",
        "out = output_volume(inn,(3,3),1,1)\n",
        "print(\"fourth in {} \\t fourth out {} >> pool34\".format(inn,out))\n",
        "inn = (out[0]//2, out[1]//2)\n",
        "out = output_volume(inn,(3,3),1,1)\n",
        "print(\"fifth in {} \\t fifth out {} >> pool5\".format(inn,out))\n",
        "inn = (out[0]//2, out[1]//2)\n",
        "out = output_volume(inn,(3,3),1,1)\n",
        "print(\"sixth in {} \\t sixth out {} >> pool6\".format(inn,out))\n",
        "out = (out[0]//2, out[1]//2)\n",
        "print(\"final output {}\".format(out))\n",
        "print(\"flatout = {} * {} = {}\".format(out[0],out[1],out[0]*out[1]))\n",
        "flatout = out[0] * out[1]\n",
        "\n",
        "class scratchNet(nn.Module):    \n",
        "    def __init__(self, flatout):\n",
        "        super(scratchNet, self).__init__()\n",
        "        self.flatout = flatout\n",
        "        # input layer ((3, H , W) --> convo layers)\n",
        "        self.con1 = nn.Conv2d(3, 56, 3, stride=1, padding=1)\n",
        "        self.con2 = nn.Conv2d(56, 56, 3, stride=1, padding=1)\n",
        "        self.con3 = nn.Conv2d(56, 112, 3, stride=1, padding=1)\n",
        "        self.con4 = nn.Conv2d(112, 112, 3, stride=1, padding=1)\n",
        "        self.con5 = nn.Conv2d(112, 224, 3, stride=1, padding=1)\n",
        "        self.con6 = nn.Conv2d(224, 200, 3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.flat = nn.Flatten()\n",
        "        self.dense1 = nn.Linear(flatout*200, flatout*20)\n",
        "        self.dense2 = nn.Linear(flatout*20, flatout*2)\n",
        "        self.out = nn.Linear(flatout*2, 50)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        #self.dropout2 = nn.Dropout(0.2)\n",
        "        #self.dropout3 = nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ## Define forward pass\n",
        "        # convolutional layers\n",
        "        x = F.relu(self.con1(x))\n",
        "        x = self.pool(F.relu(self.con2(x)))\n",
        "        x = F.relu(self.con3(x))\n",
        "        x = self.pool(F.relu(self.con4(x)))\n",
        "        x = self.pool(F.relu(self.con5(x)))\n",
        "        x = self.pool(F.relu(self.con6(x)))\n",
        "        # flatten input with a nn.Flatten layer instead of with x.view\n",
        "        x = self.flat(x)\n",
        "        x = self.dropout(x)\n",
        "        # first hidden layer\n",
        "        x = F.relu(self.dense1(x))\n",
        "        x = self.dropout(x)\n",
        "        # second hidden layer\n",
        "        x = F.relu(self.dense2(x))\n",
        "        #x = self.dropout1(x)\n",
        "        # output layer (no activation)\n",
        "        x = self.out(x)     \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oEJvTftjifG"
      },
      "source": [
        "__Question 2:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  \n",
        "\n",
        "__Answer:__  \n",
        "\n",
        "I went through a number of different designs.  The decisions I made at each step were largely decided by trial-and-error.\n",
        "\n",
        "Among the main influences on my final design was the design of the pretrained models I looked at in Step 2. The decision of make the parameters the same for all of the convolutional layers was inspired by the design of VGG19. I was interested in also implementing some \"feed-forward\" connections between layers like in the ResNet designs, but was not sure how to do so with torch (I couldn't find a \"concatenate\" layer). And in any case my design is only a few convolutional layers deep, so I decided against trying to implement that feature for now. \n",
        "\n",
        "The helper-function \"output_volume\" calculates the final output size of the stack of convolutional layers, based on their input and parameters (following the formula output-size = ((V − F + 2P)/S) + 1 from the course materials).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOa8Go2Rh1tS"
      },
      "source": [
        "# instantiate the CNN\n",
        "model_scratch = scratchNet(flatout)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    model_scratch.cuda()\n",
        "\n",
        "print(model_scratch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvR-uWSMjifG"
      },
      "source": [
        "### Define Train function\n",
        "\n",
        "(IMPLEMENTATION) Implement the Training Algorithm\n",
        "\n",
        "Implement your training algorithm in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at the filepath stored in the variable `save_path`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcfT2c7dALj4"
      },
      "source": [
        "# define train routine\n",
        "\n",
        "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
        "    \"\"\"returns trained model\"\"\"\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf \n",
        "    # detect if loss is increasing for early-stopping\n",
        "    no_loss = 0\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # initialize variables to monitor training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        # set the module to training mode\n",
        "        model.train()\n",
        "        for data, target in loaders['train']:\n",
        "            # move data to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            ## I'm not sure I understand the following instruction:\n",
        "            ## record the average training loss, using something like\n",
        "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n",
        "\n",
        "            ## find the loss and update the model parameters accordingly\n",
        "            # initialize optimizer variables to zero\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            output = model(data)\n",
        "            # calculate loss; returns batch *average*\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            # update parameters\n",
        "            optimizer.step()\n",
        "            # update total training loss for this batch\n",
        "            train_loss += loss.item()*data.size(0) \n",
        "\n",
        "        ## calculate epoch loss as loss per batch     \n",
        "        train_loss = train_loss/len(loaders['train'].dataset)\n",
        "\n",
        "        # end training if loss is nan\n",
        "        if np.isnan(loss.item()):\n",
        "            print(\"NAN loss. Ending training.\")\n",
        "            break\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        for data, target in loaders['valid']:\n",
        "            # move data to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            ## sum up total validation loss for each batch \n",
        "            # forward pass\n",
        "            output = model(data)\n",
        "            # batch loss returned as batch *average*\n",
        "            loss = criterion(output, target)\n",
        "            # update total batch loss = avg batch loss x batch size\n",
        "            valid_loss += loss.item()*data.size(0)     \n",
        "        ## calculate epoch loss as loss per batch\n",
        "        valid_loss = valid_loss/len(loaders['valid'].dataset)\n",
        "        \n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'\\\n",
        "                                    .format(epoch, train_loss, valid_loss))\n",
        "\n",
        "        ## if the validation loss has decreased, save the model to save_path\n",
        "        if valid_loss < valid_loss_min:\n",
        "            print('>>> Loss decrease of {:.9f} <<<'.format(valid_loss_min - valid_loss))\n",
        "            # save model\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            valid_loss_min = valid_loss\n",
        "            no_loss = 0\n",
        "        else:\n",
        "            no_loss += 1\n",
        "\n",
        "        ## Early stopping. End training if loss is no longer decreasing.\n",
        "        if no_loss >= 3:\n",
        "            print(\"No more loss. Terminating session.\")\n",
        "            break\n",
        "        \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylc6-rkRjifI"
      },
      "source": [
        "### (IMPLEMENTATION) Experiment with the Weight Initialization\n",
        "\n",
        "Use the code cell below to define a custom weight initialization, and then train with your weight initialization for a few epochs. Make sure that neither the training loss nor validation loss is `nan`.\n",
        "\n",
        "Later on, you will be able to see how this compares to training with PyTorch's default weight initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4n6XDLNCWpY"
      },
      "source": [
        "for p in model_scratch.parameters():\n",
        "    print(p.size()[0], np.round(p.mean().item(),2), np.round(1.0/p.std().item(), 4))\n",
        "\n",
        "torch.save(model_scratch.state_dict(), 'scratch_init.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baTpyyRimxqq"
      },
      "source": [
        "def custom_weight_init(m):\n",
        "    ## TODO: implement a weight initialization strategy\n",
        "    for p in m.parameters():\n",
        "        n = p.size(0)\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=1.0/(10*n**0.5))\n",
        "        #pass\n",
        "        #torch.nn.init.uniform_(p,-0.00001, 0.0001)\n",
        "        #torch.nn.init.normal_(p, mean=1/n, std=1.0/(n**0.5))\n",
        "        #torch.nn.init.normal_(p, mean=0.44, std=0.22)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UYJPsK-iDH9"
      },
      "source": [
        "model_scratch.apply(custom_weight_init)\n",
        "for p in model_scratch.parameters():\n",
        "    print(np.round(p.mean().item(),8), np.round(p.std().item(), 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8D6dW-fjifJ"
      },
      "source": [
        "### (IMPLEMENTATION) Train and Validate the Model\n",
        "\n",
        "Run the next code cell to train your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkuqZv8uoJ3y"
      },
      "source": [
        "#-#-# Do NOT modify the code below this line. #-#-#\n",
        "model_scratch.apply(custom_weight_init)\n",
        "optimizer_scratch = get_optimizer_scratch(model_scratch, lr=0.1)\n",
        "model_scratch = train(50, loaders_scratch, model_scratch, optimizer_scratch,\n",
        "                      criterion_scratch, use_cuda, 'ignore_scratch.pt')\n",
        "# load the model that got the best validation accuracy\n",
        "model_scratch.load_state_dict(torch.load('ignore_scratch.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp66-k2Syjcv"
      },
      "source": [
        "model_scratch.load_state_dict(torch.load('/content/google/MyDrive/Data/ignore_scratch43.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei45hxdLh-Z-"
      },
      "source": [
        "## Train:\n",
        "#model_scratch.load_state_dict(torch.load('scratch_init.pt'))\n",
        "#model_scratch.apply(custom_weight_init)\n",
        "optimizer_scratch = get_optimizer_scratch(model_scratch, lr=0.01)\n",
        "n_epochs = 10\n",
        "\n",
        "model_scratch = train(n_epochs, loaders_scratch, model_scratch, \n",
        "                      optimizer_scratch, criterion_scratch, \n",
        "                      use_cuda, 'ignore_scratch.pt')\n",
        "\n",
        "model_scratch.load_state_dict(torch.load('ignore_scratch.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUWtxF7TjifK"
      },
      "source": [
        "### (IMPLEMENTATION) Test the Model\n",
        "\n",
        "Run the code cell below to try out your model on the test dataset of landmark images. Run the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 20%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rlT_OnHjifK"
      },
      "source": [
        "#@ title Define test functions\n",
        "\n",
        "def test(loader, model, criterion, use_cuda):\n",
        "    # track test loss \n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(50))\n",
        "    class_total = list(0. for i in range(50))\n",
        "\n",
        "    model.eval() # eval mode\n",
        "\n",
        "    # iterate over test data\n",
        "    for data, target in loader:\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update  test loss \n",
        "        test_loss += loss.item()*data.size(0)\n",
        "        # convert output probabilities to predicted class\n",
        "        pct, pred = torch.max(output, 1)    \n",
        "        # compare predictions to true label\n",
        "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "        correct = np.squeeze(correct_tensor.numpy())\\\n",
        "                                     if not use_cuda\\\n",
        "                                     else np.squeeze(correct_tensor.cpu().numpy())\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(data.size(0)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1 \n",
        "\n",
        "    # calculate avg test loss\n",
        "    test_loss = test_loss/len(loader.dataset)\n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    for i in range(50):\n",
        "        if class_total[i] > 0:\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (num2class[i], \n",
        "                                                            100 * class_correct[i] / class_total[i],\n",
        "                                                            class_correct[i], \n",
        "                                                            class_total[i]))  \n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (num2class[i]))\n",
        "\n",
        "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total),\n",
        "                                                        np.sum(class_correct), \n",
        "                                                        np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IICiRkPufdq3"
      },
      "source": [
        "test(loaders['test'], model_scratch, criterion_scratch, use_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNv-TSitjifK"
      },
      "source": [
        "---\n",
        "<a id='step2'></a>\n",
        "## Step 2: Create a CNN to Classify Landmarks (using Transfer Learning)\n",
        "\n",
        "You will now use transfer learning to create a CNN that can identify landmarks from images.  Your CNN must attain at least 60% accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htx0v98csh9W"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "    torch.cuda.ipc_collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(torch.cuda.list_gpu_processes() )\n",
        "    snapshot = torch.cuda.memory_snapshot()  \n",
        "[(r['address'], r['active_size']/1e6) for r in snapshot]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_Hl5hkZ7QME"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Data Loaders for the Landmark Dataset\n",
        "\n",
        "Use the code cell below to create three separate [data loaders](http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader): one for training data, one for validation data, and one for test data. Randomly split the images located at `landmark_images/train` to create the train and validation data loaders, and use the images located at `landmark_images/test` to create the test data loader.\n",
        "\n",
        "All three of your data loaders should be accessible via a dictionary named `loaders_transfer`. Your train data loader should be at `loaders_transfer['train']`, your validation data loader should be at `loaders_transfer['valid']`, and your test data loader should be at `loaders_transfer['test']`.\n",
        "\n",
        "If you like, **you are welcome to use the same data loaders from the previous step**, when you created a CNN from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsu_QF_RjifL"
      },
      "source": [
        "### Write data loaders for training, validation, and test sets\n",
        "loaders_transfer = loaders #same loaders as loaders_scratch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JUFMZEgjifL"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
        "\n",
        "Use the next code cell to specify a [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/stable/optim.html).  Save the chosen loss function as `criterion_transfer`, and fill in the function `get_optimizer_transfer` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXj6mhdAjifL"
      },
      "source": [
        "## select loss function\n",
        "criterion_transfer = nn.CrossEntropyLoss()\n",
        "\n",
        "def get_optimizer_transfer(model, lr=0.001):\n",
        "    ## select and return optimizer, default learning rate 0.001\n",
        "    return optim.SGD(model.parameters(), lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJiMZPPaqrdT"
      },
      "source": [
        "---\n",
        "### (Not Implemented)  \"Replace Final Classifier Layer\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6H4nx-h8YA9"
      },
      "source": [
        "Once you have the pre-trained feature extractor, you just need to modify and/or add to the final, fully-connected classifier layers. In this case, we suggest that you repace the last layer in the vgg classifier group of layers. \n",
        "> This layer should see as input the number of features produced by the portion of the network that you are not changing, and produce an appropriate number of outputs for the flower classification task.\n",
        "\n",
        "You can access any layer in a pretrained network by name and (sometimes) number, i.e. `vgg16.classifier[6]` is the sixth layer in a group of layers named \"classifier\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoVOog3AqrdU",
        "cellView": "form"
      },
      "source": [
        "#@title \"Replace last layer\": see Question 4\n",
        "#n_inputs = vgg.classifier[6].in_features\n",
        "\n",
        "# new layers automatically have requires_grad = True\n",
        "#fork_layer = nn.Linear(n_inputs, len(classes))\n",
        "#VGG.classifier[6] = fork_layer\n",
        "\n",
        "# if GPU is available, move the model to GPU\n",
        "#if use_cuda:\n",
        "#    vgg.cuda()\n",
        "\n",
        "# check to see that your last layer produces the expected number of outputs\n",
        "#print(model_transfer.vgg.classifier[6], model_transfer.out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOSLoA5WjifL"
      },
      "source": [
        "### (IMPLEMENTATION) Model Architecture\n",
        "\n",
        "Use transfer learning to create a CNN to classify images of landmarks.  Use the code cell below, and save your initialized model as the variable `model_transfer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUGEucB4jifL"
      },
      "source": [
        "## Specify model architecture\n",
        "\n",
        "# Load the pretrained models from pytorch\n",
        "vgg = models.vgg19(pretrained=True)\n",
        "\n",
        "# Define model class with its own internal copy of imported model vgg\n",
        "class reVGG(nn.Module):   \n",
        "    def __init__(self, model):\n",
        "        super(reVGG, self).__init__()\n",
        "        # param 'model' is the pretrained stand-alone NN\n",
        "        self.vgg = model\n",
        "        self.relu = nn.ReLU()        \n",
        "        self.drop = nn.Dropout(0.3)  \n",
        "        self.out = nn.Linear(1000, 50)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get ReLU-regulated output from trained model\n",
        "        x = self.vgg(x)\n",
        "        x = self.relu(x) \n",
        "        x = self.drop(x)\n",
        "        # Add a final layer to go from 1000 params to 50 logits\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model_transfer = reVGG(vgg)\n",
        "\n",
        "#-#-# Do NOT modify the code below this line. #-#-#\n",
        "# if GPU is available, move the model to GPU\n",
        "if use_cuda:\n",
        "    model_transfer = model_transfer.cuda()\n",
        "\n",
        "# check to see that your last layer produces the expected number of outputs\n",
        "print(model_transfer.vgg.classifier[6], model_transfer.out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aJ_miYhyqrdT"
      },
      "source": [
        "# Deny training for all layers\n",
        "for param in model_transfer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Permit training for all the pretrained \"classifier\"/linear layers,\n",
        "# except for the first (classifier[0]) at first\n",
        "for clsfr in model_transfer.vgg.classifier[1:]:\n",
        "    for prmtr in clsfr.parameters():\n",
        "        prmtr.requires_grad = True\n",
        "\n",
        "# Turn training back on for final output layer too\n",
        "for param in model_transfer.out.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "## Check that all the layers are properly set \n",
        "## to either trainable or untrainable\n",
        "for prmtr in model_transfer.parameters():\n",
        "    print(prmtr.size(), prmtr.requires_grad)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7uYVJSvRqrdS"
      },
      "source": [
        "# print out whole model architecture\n",
        "print(model_transfer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe5lhT7QjifM"
      },
      "source": [
        "__Question 3:__ \n",
        "\n",
        "Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
        "\n",
        "__Answer:__ \n",
        "\n",
        "The most important thing about my model's architecture is that it relies on a pretrained model already proven to perform this particular classification task well. The general VGG design is a series of convolutional and pooling layers, followed by a set of densely-connected linear layers.  The pretrained model has already arrived at a set of parameters that takes in image data and classifies it.  All that had to be done to apply it to this specific task was to give it an un-trained final layer to take in the highly processed parameters of the pretrained model and let it learn to organize that ouput into the 50 different classes of the given image datasets.  \n",
        "\n",
        "The only other thing to decide was how many and which of the pretrained layers to be \"retrained\" along with the final untrained layer.  That decision was made by trial-and-error.  A two-stage process which at first permits on the final two layers to be trained at a high learning rate, followed by unlocking all six of the pretrained classifier layers for a few epochs at a much lower learning rate, seemed to work best.  This is another feature of the design that could be optimized for a better result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy-9HzYljifM"
      },
      "source": [
        "### (IMPLEMENTATION) Train and Validate the Model\n",
        "\n",
        "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_transfer.pt'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSTr8ebWjifM"
      },
      "source": [
        "## Train the model and save the best model parameters at filepath 'model_transfer.pt'\n",
        "\n",
        "# Initial hyperparams for first training set for up to 30 epochs\n",
        "optimizer_transfer = get_optimizer_transfer(model_transfer, lr=0.01)\n",
        "criterion_transfer = nn.CrossEntropyLoss()\n",
        "n_epochs = 30\n",
        "\n",
        "# Initial training session with high learning rate\n",
        "# and top classifier layer locked:\n",
        "model_transfer = train(n_epochs, loaders, model_transfer, optimizer_transfer,\n",
        "                       criterion_transfer, use_cuda, 'model_transfer.pt')\n",
        "\n",
        "#-#-# Do NOT modify the code below this line. #-#-#\n",
        "# load the model that got the best validation accuracy\n",
        "model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFMMM31dUi2k"
      },
      "source": [
        "# permit learning for first classifier layer:\n",
        "for p in model_transfer.vgg.classifier[0].parameters():\n",
        "    p.requires_grad = True\n",
        "    \n",
        "for prmtr in model_transfer.parameters():\n",
        "    print(prmtr.size(), prmtr.requires_grad)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJQMYbQvliy8"
      },
      "source": [
        "## Second training set with the other layer trainable at lower learning rate\n",
        "\n",
        "## Train up to 50 _more_ epochs at lower learning rate\n",
        "optimizer_transfer = get_optimizer_transfer(model_transfer, lr=0.0005)\n",
        "n_epochs = 50\n",
        "\n",
        "model_transfer = train(n_epochs, loaders, model_transfer, optimizer_transfer,\n",
        "                       criterion_transfer, use_cuda, 'model_transfer.pt')\n",
        "\n",
        "#-#-# Do NOT modify the code below this line. #-#-#\n",
        "# load the model that got the best validation accuracy\n",
        "model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo5a5extjifN"
      },
      "source": [
        "### (IMPLEMENTATION) Test the Model\n",
        "\n",
        "Try out your model on the test dataset of landmark images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXNojCTpjifN"
      },
      "source": [
        "test(loaders_transfer['test'], model_transfer, criterion_transfer, use_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Dc2wopcTqrdW"
      },
      "source": [
        "### Visualize Sample Test Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCLnN-q1qrdX"
      },
      "source": [
        "# obtain one batch of test images\n",
        "try:\n",
        "    images, labels = dataiter.next()\n",
        "except NameError:\n",
        "    dataiter = iter(testloader)\n",
        "    images, labels = dataiter.next()\n",
        "\n",
        "# move model inputs to cuda, if GPU available\n",
        "if use_cuda:\n",
        "    images = images.cuda()\n",
        "\n",
        "# get sample outputs\n",
        "output = model_transfer(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max(output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not use_cuda \\\n",
        "                                         else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(16, 16))\n",
        "for idx in range(9):\n",
        "    ax = fig.add_subplot(3, 3, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(np.transpose(images.cpu()[idx], (1, 2, 0)))\n",
        "    ax.set_title(\"Pred: %5s\\n[Ans: %5s]\" % (num2class[preds[idx]],\n",
        "                                            num2class[labels[idx].item()]),\n",
        "                  color=(\"blue\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SdFMX81jifN"
      },
      "source": [
        "---\n",
        "<a id='step3'></a>\n",
        "## Step 3: Write Your Landmark Prediction Algorithm\n",
        "\n",
        "Great job creating your CNN models! Now that you have put in all the hard work of creating accurate classifiers, let's define some functions to make it easy for others to use your classifiers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfcF4KLCCY28"
      },
      "source": [
        "model_transfer.load_state_dict(torch.load('/content/google/MyDrive/Data/model_transfer74.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkr2jc5w1N9A"
      },
      "source": [
        "### (IMPLEMENTATION) Write Your Algorithm, Part 1\n",
        "\n",
        "Implement the function `predict_landmarks`, which accepts a file path to an image and an integer k, and then predicts the **top k most likely landmarks**. You are **required** to use your transfer learned CNN from Step 2 to predict the landmarks.\n",
        "\n",
        "An example of the expected behavior of `predict_landmarks`:\n",
        "```\n",
        ">>> predicted_landmarks = predict_landmarks('example_image.jpg', 3)\n",
        ">>> print(predicted_landmarks)\n",
        "['Golden Gate Bridge', 'Brooklyn Bridge', 'Sydney Harbour Bridge']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo34Awv3jifN"
      },
      "source": [
        "def predict_landmarks(img_path, k):\n",
        "    ## returns the names of the top k landmarks predicted by the \n",
        "    ## learning-transfered, CNN given a single raw undistorted image\n",
        "\n",
        "    def topk(x, k):\n",
        "        # input x is the output vector for a single image as a 1D np.array\n",
        "        x = list(x.flatten())\n",
        "        top = []\n",
        "        while len(x) >= k and len(top) < k:\n",
        "            top.append(np.argmax(x))\n",
        "            x[top[-1]] = -np.inf\n",
        "        return top\n",
        "\n",
        "    img = plt.imread(img_path)\n",
        "    img = test_transforms(Image.fromarray(img))\n",
        "    scores = model_transfer(img.reshape(1,3,224,224).cuda())\n",
        "    scores = scores.cpu().detach().numpy()\n",
        "    return [num2class[t] for t in topk(x=scores[0], k=k)]\n",
        "\n",
        "# test on a sample image\n",
        "top3 = predict_landmarks('landmark_images/test/09.Golden_Gate_Bridge/190f3bae17c32c37.jpg', 3)\n",
        "print(top3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlDgEipcjifN"
      },
      "source": [
        "### (IMPLEMENTATION) Write Your Algorithm, Part 2\n",
        "\n",
        "In the code cell below, implement the function `suggest_locations`, which accepts a file path to an image as input, and then displays the image and the **top 3 most likely landmarks** as predicted by `predict_landmarks`.\n",
        "\n",
        "Some sample output for `suggest_locations` is provided below, but feel free to design your own user experience!\n",
        "![](images/sample_landmark_output.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffXAqD11jifO"
      },
      "source": [
        "def suggest_locations(img_path):\n",
        "    # get landmark predictions\n",
        "    suggestions = predict_landmarks(img_path, 3)\n",
        "    # display image with suggestions\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[])\n",
        "    ax.set_title(\"~Top 3 Suggestions~\\n1: %5s\\n2: %5s\\n3: %5s\" % (suggestions[0],\n",
        "                                                                  suggestions[1],\n",
        "                                                                  suggestions[2]))\n",
        "    plt.imshow(plt.imread(img_path))\n",
        "    return suggestions\n",
        "    \n",
        "# test on a sample image\n",
        "suggestions = suggest_locations('/content/landmark_images/test/07.Stonehenge/08907936e34608e3.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTrEVTj9jifO"
      },
      "source": [
        "### (IMPLEMENTATION) Test Your Algorithm\n",
        "\n",
        "Test your algorithm by running the `suggest_locations` function on at least four images on your computer. Feel free to use any images you like.  Feel free to use as many code cells as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPMReZ7ZjifO"
      },
      "source": [
        "suggestions = suggest_locations('/content/Solar Analemma 2018.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTxxzo9cO8_b"
      },
      "source": [
        "suggestions = suggest_locations('/content/torre del sol II.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOTJuJsfVOQZ"
      },
      "source": [
        "suggestions = suggest_locations('landmark_images/test/09.Golden_Gate_Bridge/190f3bae17c32c37.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSQXHwoEjifO"
      },
      "source": [
        "__Question 4:__ \n",
        "\n",
        "Is the output better than you expected? :) \n",
        "    Or worse? :(   \n",
        "    Provide at least three possible points of improvement for your algorithm.\n",
        "\n",
        "__Answer:__ \n",
        "\n",
        "* data: more and better (and better augmented too)\n",
        "* keep last layer and tack another on at the end w/ params 1000 --> 50 (as implemented)\n",
        "* variable learning rate? Better training \"strategy\"? \n",
        "* \"Feed-forward\" links?\n",
        "* more untrained layers?\n",
        "\n"
      ]
    }
  ]
}