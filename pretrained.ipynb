{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pretrained.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqgHU311wzMH"
      },
      "source": [
        "#Pretrained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFRGJKWKHfWS"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVLg8wpzElLF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler\n",
        "\n",
        "import os\n",
        "import random as rd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OnZEQWnElLU"
      },
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/google')\n",
        "\n",
        "zipfile.ZipFile('/content/google/MyDrive/Colab Notebooks/Udacity/deep_learning/landmark_images.zip').extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knjSFDrCElLU",
        "cellView": "form"
      },
      "source": [
        "#@title Unnecessary\n",
        "uneccessary = True\n",
        "if not uneccessary:\n",
        "    upload = True\n",
        "    if upload:\n",
        "        try:\n",
        "            train_shapes = pd.read_csv('train_shapes.csv',header=0, index_col=0)\n",
        "            test_shapes = pd.read_csv('test_shapes.csv',header=0, index_col=0)\n",
        "        except FileNotFoundError:\n",
        "            print (\"Must upload files first.\")\n",
        "\n",
        "    else:\n",
        "        #read train image file data:\n",
        "        train_path = './landmark_images/train/'\n",
        "        cat_path = [train_path + cat + '/' for cat in os.listdir(train_path)]\n",
        "        jpg_list = [[cp + jpg for jpg in os.listdir(cp)] for cp in cat_path]\n",
        "\n",
        "        train_images = []\n",
        "        for jl in jpg_list:\n",
        "            for jpg in jl:\n",
        "                train_images.append(jpg)\n",
        "\n",
        "        train_image_shapes = []\n",
        "        for imfile in train_images:\n",
        "            train_image_shapes.append(cv2.imread(imfile).shape)\n",
        "        heights = [shp[0] for shp in train_image_shapes]    \n",
        "        widths = [shp[1] for shp in train_image_shapes]\n",
        "        \n",
        "        train_images_df = pd.DataFrame(columns=['filepath','height','width'])\n",
        "        train_images_df['filepath'] = train_images\n",
        "        train_images_df['height'] = heights\n",
        "        train_images_df['width'] = widths\n",
        "        train_images_df.to_csv('train_shapes.csv')\n",
        "        \n",
        "        #read test image file data:\n",
        "        test_path = './landmark_images/test/'\n",
        "        cat_path = [test_path + cat + '/' for cat in os.listdir(test_path)]\n",
        "        jpg_list = [[cp + jpg for jpg in os.listdir(cp)] for cp in cat_path]\n",
        "\n",
        "        test_images = []\n",
        "        for jl in jpg_list:\n",
        "            for jpg in jl:\n",
        "                test_images.append(jpg)\n",
        "\n",
        "        test_image_shapes = []\n",
        "        for imfile in test_images:\n",
        "            test_image_shapes.append(cv2.imread(imfile).shape)\n",
        "        heights = [shp[0] for shp in test_image_shapes]    \n",
        "        widths = [shp[1] for shp in test_image_shapes] \n",
        "\n",
        "        test_images_df = pd.DataFrame(columns=['filepath','height','width'])\n",
        "        test_images_df['filepath'] = test_images\n",
        "        test_images_df['height'] = heights\n",
        "        test_images_df['width'] = widths\n",
        "        test_images_df.to_csv('test_shapes.csv')\n",
        "\n",
        "        #cleanup\n",
        "        del train_images, train_image_shapes, train_path\n",
        "        del cat_path, jpg_list, widths, heights\n",
        "        del test_images, test_image_shapes, test_path\n",
        "        #del train_images_df, test_images_df\n",
        "\n",
        "    def quick_label(row):\n",
        "        label = row['filepath'][-40:-20] + \"\\nWidth: \" + str(row['width']) + \"  Height: \" + str(row['height'])\n",
        "        return label\n",
        "\n",
        "    choices = rd.sample(list(train_shapes.index), 4)\n",
        "    paths = [(train_shapes.iloc[s])['filepath'] for s in choices]\n",
        "    labels = [quick_label(train_shapes.iloc[s]) for s in choices]\n",
        "    imgs_bgr = [plt.imread(p) for p in paths]\n",
        "    imgs_rgb = [bgr2rgb(im) for im in imgs_bgr] \n",
        "\n",
        "    fig, axes = plt.subplots(figsize=(18,16), nrows=2, ncols=2)\n",
        "    axes = axes.reshape(4)\n",
        "    for ii, im in enumerate(imgs_rgb):\n",
        "        ax0 = axes[ii] \n",
        "        ax0.set_title(labels[ii])\n",
        "        ax0.imshow(im)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihxcF63pElLW"
      },
      "source": [
        "#@ title Image transpose\n",
        "def bgr2rgb(im_bgr):\n",
        "    # because cv2.imread() returns BGR image format\n",
        "    im_rgb = im_bgr.copy()\n",
        "    im_rgb[:,:,0] = im_bgr[:,:,2]\n",
        "    im_rgb[:,:,1] = im_bgr[:,:,1]\n",
        "    im_rgb[:,:,2] = im_bgr[:,:,0]\n",
        "    return im_rgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp-V_gHqXba7"
      },
      "source": [
        "(3997//25, 3997%25), (999//25, 999%25), (1250//25, 1250%25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qyBEa2DYVOe"
      },
      "source": [
        "train_shapes = pd.read_csv('train_shapes.csv',header=0, index_col=0)\n",
        "test_shapes = pd.read_csv('test_shapes.csv',header=0, index_col=0)\n",
        "# Cat nums to labels\n",
        "classvals = list(set([(str(pair[0]),pair[1]) for pair in test_shapes[['labnum','label']].values]))\n",
        "num2class = {int(x):y for x,y in classvals}        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ia7LyEMElLX"
      },
      "source": [
        "# image directory location\n",
        "data_dir = '/content/landmark_images'\n",
        "\n",
        "batch_size = 25\n",
        "\n",
        "# set target height and width for all images\n",
        "H = 300\n",
        "W = 400\n",
        "h0 = 450\n",
        "w0 = 600\n",
        "h1 = 600\n",
        "w1 = 800\n",
        "\n",
        "train_transforms = transforms.Compose( [transforms.Resize((h1, w1)),\n",
        "                                        transforms.RandomCrop((h0,w0)),\n",
        "                                        transforms.RandomRotation(20, \n",
        "                                                                  fill=np.random.randint(100,201)),\n",
        "                                        transforms.CenterCrop((H,W)),\n",
        "                                        transforms.ToTensor()])\n",
        "\n",
        "test_transforms = transforms.Compose([ transforms.Resize((H, W)),\n",
        "                                       transforms.ToTensor()])\n",
        "\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
        "\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(0.2 * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, \n",
        "                                          batch_size=batch_size,\n",
        "                                          sampler=train_sampler)\n",
        "\n",
        "validloader = torch.utils.data.DataLoader(train_data, \n",
        "                                          batch_size=batch_size,\n",
        "                                          sampler=valid_sampler)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_data, \n",
        "                                         batch_size=batch_size,\n",
        "                                         sampler=test_sampler)\n",
        "\n",
        "loaders_scratch = {'train': trainloader, 'valid': validloader, 'test': testloader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXF7QM5DqrdR"
      },
      "source": [
        "---\n",
        "## Load VGG16 as revgg\n",
        "\n",
        "To define a model for training we'll follow these steps:\n",
        "1. Load in a pre-trained VGG16 model\n",
        "2. \"Freeze\" all the parameters, so the net acts as a fixed feature extractor \n",
        "3. Remove the last layer\n",
        "4. Replace the last layer with a linear classifier of our own\n",
        "\n",
        "**Freezing simply means that the parameters in the pre-trained model will *not* change during training.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7uYVJSvRqrdS"
      },
      "source": [
        "# Load the pretrained models from pytorch\n",
        "revgg = models.vgg16(pretrained=True)\n",
        "\n",
        "# print out the model structure\n",
        "print(revgg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aJ_miYhyqrdT"
      },
      "source": [
        "# Freeze training for all \"features\" layers\n",
        "for param in revgg.features.parameters():\n",
        "    param.requires_grad = False\n",
        "    print(param.size(), param.requires_grad) \n",
        "    \n",
        "#for clsfr in revgg.classifier:\n",
        "#    for prmtr in clsfr.parameters():\n",
        "#        prmtr.requires_grad = True\n",
        "#        print(prmtr.size(), prmtr.requires_grad) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJiMZPPaqrdT"
      },
      "source": [
        "---\n",
        "### Final Classifier Layer\n",
        "\n",
        "Once you have the pre-trained feature extractor, you just need to modify and/or add to the final, fully-connected classifier layers. In this case, we suggest that you repace the last layer in the vgg classifier group of layers. \n",
        "> This layer should see as input the number of features produced by the portion of the network that you are not changing, and produce an appropriate number of outputs for the flower classification task.\n",
        "\n",
        "You can access any layer in a pretrained network by name and (sometimes) number, i.e. `vgg16.classifier[6]` is the sixth layer in a group of layers named \"classifier\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xBUWZ4KJS9N"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoVOog3AqrdU"
      },
      "source": [
        "n_inputs = revgg.classifier[6].in_features\n",
        "\n",
        "# new layers automatically have requires_grad = True\n",
        "fork_layer = nn.Linear(n_inputs, 50)\n",
        "#last_layer = nn.Linear(50*len(classes), len(classes))\n",
        "\n",
        "revgg.classifier[6] = fork_layer\n",
        "\n",
        "# if GPU is available, move the model to GPU\n",
        "if use_cuda:\n",
        "    revgg.cuda()\n",
        "\n",
        "# check to see that your last layer produces the expected number of outputs\n",
        "print(revgg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3J_wxZ-_D2o"
      },
      "source": [
        "# Check for all layers if trainable on training\n",
        "for param in revgg.features.parameters():\n",
        "    print(param.size(), param.requires_grad) \n",
        "print(\"\\n\")\n",
        "for param in revgg.classifier.parameters():\n",
        "    print(param.size(), param.requires_grad) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "QgFElRorqrdU"
      },
      "source": [
        "#import torch.optim as optim\n",
        "\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer (stochastic gradient descent) and learning rate = 0.001\n",
        "vgg_optimizer = optim.SGD(revgg.classifier.parameters(), lr=0.005)\n",
        "\n",
        "for p in revgg.classifier[6].parameters():\n",
        "    n = p.size(0)\n",
        "    torch.nn.init.normal_(p, mean=0.5, std=0.5)#std=1.0/(n**0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1ehkG7gqrdU"
      },
      "source": [
        "---\n",
        "### Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXntNRgaqE_V"
      },
      "source": [
        "#@ title retrain def\n",
        "\n",
        "def retrain(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
        "    \"\"\"returns retrained pretrained-model\"\"\"\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf \n",
        "    no_loss = 0\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # initialize variables to monitor training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        # set the module to training mode\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()*data.size(0)       \n",
        "        train_loss = train_loss/len(loaders['train'].dataset) \n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            valid_loss += loss.item()*data.size(0)      \n",
        "        valid_loss = valid_loss/len(loaders['valid'].dataset)\n",
        "\n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "                                                                    epoch, \n",
        "                                                                    train_loss,\n",
        "                                                                    valid_loss\n",
        "                                                                    ))\n",
        "        if valid_loss < valid_loss_min:\n",
        "            print('* Loss decrease *'.format(valid_loss_min, valid_loss))\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            valid_loss_min = valid_loss\n",
        "            no_loss = 0\n",
        "        else:\n",
        "            no_loss += 1\n",
        "        \n",
        "        if no_loss >= 3:\n",
        "            print(\"=== Loss is no longer decreasing. Ending training. ===\")\n",
        "            break\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGUJWZAXs_bz"
      },
      "source": [
        "n_epochs = 20\n",
        "revgg = retrain(n_epochs, loaders_scratch, \n",
        "                revgg, vgg_optimizer, criterion, \n",
        "                use_cuda, 'revgg_last3.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1-0jgGFqrdV"
      },
      "source": [
        "---\n",
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb0vVdzlqrdW"
      },
      "source": [
        "\n",
        "#batch_size=batch_size//2\n",
        "#testloader = torch.utils.data.DataLoader(test_data, \n",
        "#                                         batch_size=batch_size,\n",
        "#                                         sampler=test_sampler)\n",
        "\n",
        "# track test loss \n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(50))\n",
        "class_total = list(0. for i in range(50))\n",
        "\n",
        "revgg.eval() # eval mode\n",
        "\n",
        "# iterate over test data\n",
        "for data, target in testloader:\n",
        "    if use_cuda:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    output = revgg(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update  test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy())\\\n",
        "              if not use_cuda\\\n",
        "              else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(data.size(0)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate avg test loss\n",
        "test_loss = test_loss/len(testloader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(50):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            num2class[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (num2class[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Dc2wopcTqrdW"
      },
      "source": [
        "### Visualize Sample Test Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCLnN-q1qrdX"
      },
      "source": [
        "# obtain one batch of test images\n",
        "try:\n",
        "    images, labels = dataiter.next()\n",
        "except NameError:\n",
        "    dataiter = iter(testloader)\n",
        "    images, labels = dataiter.next()\n",
        "images.numpy()\n",
        "\n",
        "# move model inputs to cuda, if GPU available\n",
        "if use_cuda:\n",
        "    images = images.cuda()\n",
        "\n",
        "# get sample outputs\n",
        "output = revgg(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max( output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not use_cuda else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(18, 20))\n",
        "for idx in np.arange(12):\n",
        "    ax = fig.add_subplot(4, 12/4, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(np.transpose(images.cpu()[idx], (1, 2, 0)))\n",
        "    ax.set_title(\"{}\\n({})\".format(num2class[preds[idx]], num2class[labels[idx].item()]),\n",
        "                                  color=(\"blue\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KENtthtzAzCB"
      },
      "source": [
        "---\n",
        "## Load RESNET as reres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SqwiHGyqCx93"
      },
      "source": [
        "# Load the pretrained models from pytorch\n",
        "reres = models.resnet50(pretrained=True)\n",
        "\n",
        "# print out the model structure\n",
        "print(reres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rg45akosGSu"
      },
      "source": [
        "for fcp in reres.fc.parameters():\n",
        "    print(p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "EUAsYngFCx-B"
      },
      "source": [
        "# Freeze training for all convolutional layers\n",
        "for param in reres.parameters():\n",
        "    param.requires_grad = False\n",
        "    print(param.size(), param.requires_grad) \n",
        "\n",
        "# Don't freeze training for the linear layers \n",
        "#for clsfr in reres.classifier:\n",
        "#    for prmtr in clsfr.parameters():\n",
        "#        prmtr.requires_grad = True\n",
        "#        print(prmtr.size(), prmtr.requires_grad) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDtwvmZlAzCD"
      },
      "source": [
        "n_inputs = reres.fc.in_features\n",
        "\n",
        "# new layers automatically have requires_grad = True\n",
        "fork_layer = nn.Linear(n_inputs, 50))\n",
        "\n",
        "reres.fc = fork_layer\n",
        "\n",
        "# if GPU is available, move the model to GPU\n",
        "if use_cuda:\n",
        "    reres.cuda()\n",
        "\n",
        "# check to see that your last layer produces the expected number of outputs\n",
        "print(reres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbRcnBk0AzCE"
      },
      "source": [
        "# Check for all layers if trainable on training\n",
        "for param in reres.features.parameters():\n",
        "    print(param.size(), param.requires_grad) \n",
        "print(\"\\n\")\n",
        "for param in reres.classifier.parameters():\n",
        "    print(param.size(), param.requires_grad) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MK2WB-viAzCE"
      },
      "source": [
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Only linear classifier layers go to optimizer\n",
        "res_optimizer = optim.SGD(reres.classifier.parameters(), lr=0.01)\n",
        "\n",
        "# Initialize weights for classifier layers\n",
        "for p in reres.classifier[6].parameters():\n",
        "    n = p.size(0)\n",
        "    torch.nn.init.normal_(p, mean=0.2, std=1.0/(n**0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvnp2PNiJYz7"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw8UnGytAzCF"
      },
      "source": [
        "---\n",
        "### Train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmOgr2UjAzCG"
      },
      "source": [
        "#@ title retrain def\n",
        "\n",
        "def retrain(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
        "    \"\"\"returns retrained pretrained-model\"\"\"\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf \n",
        "    no_loss = 0\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # initialize variables to monitor training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        # set the module to training mode\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()*data.size(0)       \n",
        "        train_loss = train_loss/len(loaders['train'].dataset) \n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            valid_loss += loss.item()*data.size(0)      \n",
        "        valid_loss = valid_loss/len(loaders['valid'].dataset)\n",
        "\n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "                                                                    epoch, \n",
        "                                                                    train_loss,\n",
        "                                                                    valid_loss))\n",
        "        if valid_loss < valid_loss_min:\n",
        "            print('* Loss decrease *'.format(valid_loss_min, valid_loss))\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            valid_loss_min = valid_loss\n",
        "        else:\n",
        "            no_loss += 1\n",
        "        \n",
        "        if no_loss >= 3:\n",
        "            print(\"=== Loss is no longer decreasing. Ending training. ===\")\n",
        "            break\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgkPtUF4AzCG"
      },
      "source": [
        "reres = retrain(n_epochs, loaders_scratch, \n",
        "                reres, vgg_optimizer, criterion, \n",
        "                use_cuda, 'reres_last3.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiJczCGnAzCH"
      },
      "source": [
        "---\n",
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcz2N6n5AzCH"
      },
      "source": [
        "\n",
        "batch_size=15\n",
        "testloader = torch.utils.data.DataLoader(test_data, \n",
        "                                         batch_size=batch_size,\n",
        "                                         sampler=test_sampler)\n",
        "\n",
        "# track test loss \n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(50))\n",
        "class_total = list(0. for i in range(50))\n",
        "\n",
        "reres.eval() # eval mode\n",
        "\n",
        "# iterate over test data\n",
        "for data, target in testloader:\n",
        "    if use_cuda:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    output = reres(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update  test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy())\\\n",
        "              if not use_cuda\\\n",
        "              else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(data.size(0)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate avg test loss\n",
        "test_loss = test_loss/len(testloader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(50):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            num2class[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (num2class[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "T2jd98RAAzCH"
      },
      "source": [
        "### Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8It-KuDAzCI"
      },
      "source": [
        "# obtain one batch of test images\n",
        "try:\n",
        "    images, labels = dataiter.next()\n",
        "except NameError:\n",
        "    dataiter = iter(validloader)\n",
        "    images, labels = dataiter.next()\n",
        "images.numpy()\n",
        "\n",
        "# move model inputs to cuda, if GPU available\n",
        "if use_cuda:\n",
        "    images = images.cuda()\n",
        "\n",
        "# get sample outputs\n",
        "output = reres(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max( output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not use_cuda else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(18, 25))\n",
        "for idx in np.arange(15):\n",
        "    ax = fig.add_subplot(5, 15/5, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(np.transpose(images.cpu()[idx], (1, 2, 0)))\n",
        "    ax.set_title(\"{}\\n({})\".format(num2class[preds[idx]], num2class[labels[idx].item()]),\n",
        "                                  color=(\"blue\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmB6VfpRtDDE"
      },
      "source": [
        "## Alexnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgIyYQZfrrbG"
      },
      "source": [
        "anet = models.alexnet(pretrained=True)\n",
        "print (anet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y_1Hnaub1ht"
      },
      "source": [
        "# Keras model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3DoMCPOH8ws"
      },
      "source": [
        "## load data and modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC9W_KXqb6gA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import random as rd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy as cat_entropy\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy as sparse_cat_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FqSLXiSd2FR"
      },
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/Udacity/deep_learning/landmark_images.zip').extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHGYTdaL2lMu"
      },
      "source": [
        "try:\n",
        "    train_shapes = pd.read_csv('train_shapes.csv',header=0, index_col=0)\n",
        "    test_shapes = pd.read_csv('test_shapes.csv',header=0, index_col=0)\n",
        "except FileNotFoundError:\n",
        "    print (\"Must upload train_shapes.csv and test_shapes.csv first.\")\n",
        "\n",
        "if False:\n",
        "    \n",
        "    #read train image file data:\n",
        "    train_path = './landmark_images/train/'\n",
        "    cat_path = [train_path + cat + '/' for cat in os.listdir(train_path)]\n",
        "    cat_list = [cat for cat in os.listdir(train_path)]\n",
        "    cats, train_images = [], []\n",
        "    for c in cat_list:\n",
        "        cp = str(train_path + c + '/') \n",
        "        for imfile in os.listdir(cp):\n",
        "            train_images.append(str(cp + imfile))\n",
        "            cats.append(c)\n",
        "\n",
        "    train_image_shapes = []\n",
        "    for imfile in train_images:\n",
        "        train_image_shapes.append(plt.imread(imfile).shape)\n",
        "    heights = [shp[0] for shp in train_image_shapes]    \n",
        "    widths = [shp[1] for shp in train_image_shapes]\n",
        "\n",
        "    train_images_df = pd.DataFrame(columns=['labnum','label',\n",
        "                                            'height','width',\n",
        "                                            'filepath'])\n",
        "    train_images_df['labnum'] = [int(c[:2]) for c in cats]\n",
        "    train_images_df['label'] = [c[3:] for c in cats]\n",
        "    train_images_df['height'] = heights\n",
        "    train_images_df['width'] = widths\n",
        "    train_images_df['filepath'] = train_images\n",
        "\n",
        "    #read test image file data:\n",
        "    test_path = './landmark_images/test/'\n",
        "    cat_path = [test_path + cat + '/' for cat in os.listdir(test_path)]\n",
        "    cat_list = [cat for cat in os.listdir(test_path)]\n",
        "    cats, test_images = [], []\n",
        "    for c in cat_list:\n",
        "        cp = str(test_path + c + '/') \n",
        "        for imfile in os.listdir(cp):\n",
        "            test_images.append(str(cp + imfile))\n",
        "            cats.append(c)\n",
        "\n",
        "    test_image_shapes = []\n",
        "    for imfile in test_images:\n",
        "        test_image_shapes.append(plt.imread(imfile).shape)\n",
        "    heights = [shp[0] for shp in test_image_shapes]    \n",
        "    widths = [shp[1] for shp in test_image_shapes]\n",
        "\n",
        "    test_images_df = pd.DataFrame(columns=['labnum','label',\n",
        "                                            'height','width',\n",
        "                                            'filepath'])\n",
        "    test_images_df['labnum'] = [int(c[:2]) for c in cats]\n",
        "    test_images_df['label'] = [c[3:] for c in cats]\n",
        "    test_images_df['height'] = heights\n",
        "    test_images_df['width'] = widths\n",
        "    test_images_df['filepath'] = test_images\n",
        "\n",
        "    #save dfs\n",
        "    train_images_df.to_csv('train_shapes.csv')\n",
        "    test_images_df.to_csv('test_shapes.csv')\n",
        "\n",
        "    #cleanup\n",
        "    del train_images, train_image_shapes, train_path\n",
        "    del test_images, test_image_shapes, test_path\n",
        "    del cats, cat_list, cat_path, widths, heights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jzYMZxAHmeA"
      },
      "source": [
        "## Only want exaclty 600x800 images\n",
        "testable_df = test_shapes.loc[lambda df: df['width'] == 800].loc[lambda df: df['height'] == 600]\n",
        "trainable_df = train_shapes.loc[lambda df: df['width'] == 800].loc[lambda df: df['height'] == 600]\n",
        "\n",
        "## Make class dict\n",
        "classvals = list(set([(str(pair[0]),pair[1]) for pair in test_shapes[['labnum','label']].values]))\n",
        "num2class = {int(x):y for x,y in classvals}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhOtmPYjJXHr"
      },
      "source": [
        "f = plt.figure(num=1, figsize=(20,20))\n",
        "for i in range(3):\n",
        "    randix = rd.choice(list(testable_df.index))\n",
        "    img = plt.imread(test_shapes.iloc[randix]['filepath'])/255.\n",
        "    tar = test_shapes.iloc[randix]['labnum']\n",
        "\n",
        "    ax = f.add_subplot(1,3,i+1)\n",
        "    ax.set_title(\"Class #\"+str(tar)+\" \"+num2class[tar])\n",
        "    ax.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fic3YAARH11N"
      },
      "source": [
        "## experimental models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5MXNYt2H1du",
        "cellView": "form"
      },
      "source": [
        "#@title Experimental Keras Model\n",
        "\n",
        "input_layer = layers.Input(shape=(600,800,3))\n",
        "z = layers.Conv2D(10, 3, \n",
        "                  strides=2,\n",
        "                  activation='relu',\n",
        "                  #kernel_initializer='uniform',\n",
        "                  use_bias=True,\n",
        "                  padding='same')(input_layer)\n",
        "\n",
        "y = layers.Conv2D(10, 3, \n",
        "                  strides=2,\n",
        "                  activation='relu',\n",
        "                  #kernel_initializer='normal',\n",
        "                  use_bias=True,\n",
        "                  padding='same')(z)\n",
        "\n",
        "x = layers.Conv2D(20, 7, \n",
        "                  strides=1,\n",
        "                  activation='relu',\n",
        "                  use_bias=True,\n",
        "                  padding='valid')(y)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Conv2D(30, 5, \n",
        "                  strides=1,\n",
        "                  activation='relu',\n",
        "                  use_bias=True,\n",
        "                  padding='valid')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Conv2D(40, 3, \n",
        "                  strides=1,\n",
        "                  activation='relu',\n",
        "                  use_bias=True,\n",
        "                  padding='valid')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Conv2D(60, 3, \n",
        "                  strides=1,\n",
        "                  activation='relu',\n",
        "                  use_bias=True,\n",
        "                  padding='valid')(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(1000, activation='relu')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "output_layer = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "shrinkX = Model(input_layer, y)\n",
        "modelX = Model(input_layer, output_layer)\n",
        "modelX.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhnyesCBnQFP"
      },
      "source": [
        "def scale(a):\n",
        "    return (a-a.min())/(a.max()-a.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5VL_7C3hPQf"
      },
      "source": [
        "f = plt.figure(num=2,figsize=(20,5))\n",
        "sp1 = f.add_subplot(1,4,1)\n",
        "sp2 = f.add_subplot(1,4,2)\n",
        "sp3 = f.add_subplot(1,4,3)\n",
        "sp4 = f.add_subplot(1,4,4)\n",
        "\n",
        "randix = rd.choice(list(trainable_df.index))\n",
        "img = plt.imread(train_shapes.iloc[randix]['filepath'])/255.\n",
        "tar = train_shapes.iloc[randix]['labnum']\n",
        "\n",
        "out_img = shrinkX(np.expand_dims(img,0))\n",
        "out1 = out_img.numpy()[0,:,:,0:3]\n",
        "out2 = out_img.numpy()[0,:,:,5:8]\n",
        "out3 = out_img.numpy()[0,:,:,7:10]\n",
        "\n",
        "\n",
        "sp1.set_title(\"Class #\"+str(tar)+\" \"+num2class[tar])\n",
        "sp1.imshow(scale(img))\n",
        "sp2.set_title(\"Model Output One\")\n",
        "sp2.imshow(scale(out1))\n",
        "sp3.set_title(\"Model Output Two\")\n",
        "sp3.imshow(scale(out2))\n",
        "sp4.set_title(\"Model Output Three\")\n",
        "sp4.imshow(scale(out3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO5gf2WfmEBE"
      },
      "source": [
        "print(\"min\\t max\\t avg\\t std\")\n",
        "for i in range(10):  \n",
        "    print(\"{:.4f}\\t {:.4f}\\t {:.4f}\\t {:.4f}\".format(   out_img.numpy()[0,:,:,i].min(), \n",
        "                                                        out_img.numpy()[0,:,:,i].max(),\n",
        "                                                        out_img.numpy()[0,:,:,i].mean(),\n",
        "                                                        out_img.numpy()[0,:,:,i].std()) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1-pTNYE-h3f"
      },
      "source": [
        "#### build and train fnxs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkknoBobJ4R5"
      },
      "source": [
        "#traintupleDG, testupleDG = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zo9wjSTBtC7"
      },
      "source": [
        "## Initiate hyperparameters\n",
        "epochs = 5\n",
        "learning_rate = 0.01\n",
        "batch_size = 40\n",
        "validation_batch_size = 20\n",
        "\n",
        "## Construct datasets\n",
        "x_train = np.array([plt.imread(fp) for fp in trainable_df[\"filepath\"]])/255.\n",
        "y_train = float(trainable_df[[\"labnum\"]].values.squeeze())\n",
        "\n",
        "x_test = np.array([plt.imread(fp) for fp in testable_df[\"filepath\"]])/255.\n",
        "y_test = float(testable_df[[\"labnum\"]].values.squeeze())\n",
        "\n",
        "traintuple = (x_train[:4], y_train[:4])\n",
        "testtuple = (x_test[:4], y_test[:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cGHd2HVb1MD"
      },
      "source": [
        "del x_test\n",
        "del y_test\n",
        "del x_train\n",
        "del y_train\n",
        "del traintuple\n",
        "del testtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtngjIXgUvws",
        "cellView": "form"
      },
      "source": [
        "#@title Define functions for building, training, compiling and evaluating models\n",
        "\n",
        "def build_model(verbose=False):\n",
        "    '''Builds a model and returns it uncompiled\n",
        "\n",
        "    '''\n",
        "    input_layer = layers.Input(shape=(600,800,3))\n",
        "    z = layers.Conv2D(10, 3, \n",
        "                    strides=2,\n",
        "                    activation='relu',\n",
        "                    #kernel_initializer='uniform',\n",
        "                    use_bias=True,\n",
        "                    padding='same')(input_layer)\n",
        "\n",
        "    y = layers.Conv2D(10, 3, \n",
        "                    strides=2,\n",
        "                    activation='relu',\n",
        "                    #kernel_initializer='normal',\n",
        "                    use_bias=True,\n",
        "                    padding='same')(z)\n",
        "\n",
        "    x = layers.Conv2D(20, 7, \n",
        "                    strides=1,\n",
        "                    activation='relu',\n",
        "                    use_bias=True,\n",
        "                    padding='valid')(y)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = layers.Conv2D(30, 5, \n",
        "                    strides=1,\n",
        "                    activation='relu',\n",
        "                    use_bias=True,\n",
        "                    padding='valid')(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = layers.Conv2D(40, 3, \n",
        "                    strides=1,\n",
        "                    activation='relu',\n",
        "                    use_bias=True,\n",
        "                    padding='valid')(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = layers.Conv2D(60, 3, \n",
        "                    strides=1,\n",
        "                    activation='relu',\n",
        "                    use_bias=True,\n",
        "                    padding='valid')(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(1000, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    output_layer = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "    return Model(input_layer, output_layer)\n",
        "\n",
        "## Same training loop for all models\n",
        "def train(model, traintuple, valtuple, epochs=epochs):\n",
        "    '''Train a model on the given sets of data\n",
        "        Params: the given model,\n",
        "                the train data as a tuple of x,y,\n",
        "                the test data as a tuple of x,y\n",
        "        Returns: a dictionary of metric values after each epoch of training\n",
        "    '''\n",
        "    (x_train, y_train) = traintuple    \n",
        "    history = model.fit(x=x_train,\n",
        "                        y=y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=valtuple,\n",
        "                        validation_batch_size=validation_batch_size,\n",
        "                        epochs=epochs,  \n",
        "                        verbose=1)   \n",
        "    return history.history\n",
        "print(\"Loaded function train(model, traintuple, testuple, epochs=epochs)\")\n",
        "\n",
        "## All models are compiled the same\n",
        "def compile_model(model):    \n",
        "    model.compile(  loss=sparse_cat_entropy,\n",
        "                    optimizer=SGD(learning_rate=learning_rate),                    \n",
        "                    metrics=['acc'])\n",
        "    print (\"Compiled model\", model.name)\n",
        "print(\"loaded function compile_model(model)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7okEhFBLQWit"
      },
      "source": [
        "def build_mini(verbose=False):\n",
        "    '''Builds a minimal experimental model and returns it uncompiled\n",
        "\n",
        "    '''\n",
        "    input_layer = layers.Input(shape=(600,800,3))\n",
        "    x = layers.Conv2D(10, 3, \n",
        "                    strides=2,\n",
        "                    activation='relu',\n",
        "                    use_bias=True,\n",
        "                    padding='same')(input_layer)\n",
        "\n",
        "    x = layers.MaxPooling2D(3)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    output_layer = layers.Dense(50, activation='softmax')(x)\n",
        "\n",
        "    return Model(input_layer, output_layer, name='minimodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaAnv1W3RVCW"
      },
      "source": [
        "def qtrain(model, traintuple, valtuple, epochs=epochs):\n",
        "    '''Train a model on the given sets of data\n",
        "        Params: the given model,\n",
        "                the train data as a tuple of x,y,\n",
        "                the test data as a tuple of x,y\n",
        "        Returns: a dictionary of metric values after each epoch of training\n",
        "    '''\n",
        "    (x_train, y_train) = traintuple    \n",
        "    history = model.fit(x=x_train,\n",
        "                        y=y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=valtuple,\n",
        "                        validation_batch_size=validation_batch_size,\n",
        "                        epochs=epochs,  \n",
        "                        verbose=1)   \n",
        "    return history.history\n",
        "\n",
        "def qcompile_model(model):    \n",
        "    model.compile(  loss=sparse_cat_entropy,\n",
        "                    optimizer=SGD(learning_rate=learning_rate))#,                    \n",
        "                    #metrics=['acc'])\n",
        "    print (\"Compiled model\", model.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wFhydCbIgOV"
      },
      "source": [
        "epochs = 1\n",
        "learning_rate = 0.01\n",
        "batch_size = 2\n",
        "validation_batch_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP2VqeELLpGX"
      },
      "source": [
        "### Build\n",
        "mm = build_mini()\n",
        "\n",
        "### Compile\n",
        "qcompile_model(mm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PoiiiIgJFiT"
      },
      "source": [
        "######### Print out summary table\n",
        "print(mm.summary(),\"\\n\")\n",
        "\n",
        "######## Plot model diagrams\n",
        "tf.keras.utils.plot_model(mm, \n",
        "                          show_layer_names=True, \n",
        "                          show_shapes=True, \n",
        "                          to_file=\"mm.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6XfSpmr6X2n"
      },
      "source": [
        "### Train\n",
        "train_stats = qtrain(mm, traintuple, testtuple, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0p3SmHkY-We"
      },
      "source": [
        "randix = rd.choice(list(trainable_df.index))\n",
        "img = plt.imread(train_shapes.iloc[randix]['filepath'])/255.\n",
        "tar = train_shapes.iloc[randix]['labnum']\n",
        "\n",
        "out = np.argmax(mm(np.expand_dims(img,0)).numpy().squeeze())\n",
        "print(\"target\",tar,\"\\toutput\", out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GMD2GO0awLv"
      },
      "source": [
        "traintuple[1].dtype"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}